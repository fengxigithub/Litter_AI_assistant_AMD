#!/usr/bin/env python3
"""
æ–‡æ¡£å¤„ç†å™¨ - é›†æˆåˆ°ç°æœ‰é¡¹ç›®
"""

import os
import json
import re
from pathlib import Path
from typing import List, Dict


class DocumentProcessor:
    """è½»é‡çº§æ–‡æ¡£å¤„ç†å™¨"""

    def __init__(self, base_dir: str = None):
        """
        åˆå§‹åŒ–

        Args:
            base_dir: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„ï¼ŒNoneåˆ™è‡ªåŠ¨æ£€æµ‹
        """
        if base_dir:
            self.project_root = Path(base_dir)
        else:
            # è‡ªåŠ¨æ£€æµ‹é¡¹ç›®æ ¹ç›®å½•ï¼ˆè™šæ‹Ÿç¯å¢ƒæ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•ï¼‰
            current_file = Path(__file__).resolve()
            # å‘ä¸Šæ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼ˆåŒ…å«srcç›®å½•çš„ç›®å½•ï¼‰
            while current_file.parent.name != 'src' and current_file.parent != current_file:
                current_file = current_file.parent
            self.project_root = current_file.parent.parent

        # è®¾ç½®ç›®å½•
        self.documents_dir = self.project_root / "documents"
        self.training_dir = self.project_root / "data"  # å¤ç”¨ç°æœ‰dataç›®å½•
        self.models_dir = self.project_root / "models"

        # åˆ›å»ºç›®å½•
        self.documents_dir.mkdir(exist_ok=True)
        self.training_dir.mkdir(exist_ok=True)
        self.models_dir.mkdir(exist_ok=True)

        print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {self.project_root}")
        print(f"ğŸ“ æ–‡æ¡£ç›®å½•: {self.documents_dir}")
        print(f"ğŸ“ è®­ç»ƒæ•°æ®: {self.training_dir}")

    def extract_text_from_file(self, file_path: Path) -> str:
        """ä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬ï¼ˆæ”¯æŒtxtå’Œç®€å•æ–‡æœ¬ï¼‰"""
        try:
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            suffix = file_path.suffix.lower()

            if suffix == '.txt':
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            elif suffix == '.pdf':
                # å°è¯•ä½¿ç”¨PyPDF2
                try:
                    import PyPDF2
                    text = ""
                    with open(file_path, 'rb') as file:
                        reader = PyPDF2.PdfReader(file)
                        for page in reader.pages:
                            text += page.extract_text() + "\n"
                    return text
                except ImportError:
                    print("âš ï¸  éœ€è¦å®‰è£…PyPDF2: pip install PyPDF2")
                    return ""
            elif suffix in ['.docx', '.doc']:
                # å°è¯•ä½¿ç”¨python-docx
                try:
                    from docx import Document
                    doc = Document(file_path)
                    return "\n".join([para.text for para in doc.paragraphs])
                except ImportError:
                    print("âš ï¸  éœ€è¦å®‰è£…python-docx: pip install python-docx")
                    return ""
            else:
                # å°è¯•æŒ‰æ–‡æœ¬æ–‡ä»¶è¯»å–
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        return f.read()
                except:
                    print(f"âš ï¸  ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}")
                    return ""

        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return ""

    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        if not text:
            return ""

        # ç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡å’Œæ ‡ç‚¹
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\sï¼Œã€‚ï¼Ÿï¼ï¼šï¼›"\'()ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€.,!?;:]', ' ', text)
        return text.strip()

    def split_into_sections(self, text: str, min_section_length: int = 200) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆæœ‰æ„ä¹‰çš„æ®µè½"""
        sections = []

        # æŒ‰å¥å­åˆ†å‰²ï¼ˆä¸­æ–‡æ ‡ç‚¹ï¼‰
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ!?\.]\s*', text)

        current_section = ""
        for sentence in sentences:
            if not sentence.strip():
                continue

            current_section += sentence + "ã€‚"

            # å¦‚æœè¾¾åˆ°æœ€å°é•¿åº¦ï¼Œå¼€å§‹æ–°æ®µè½
            if len(current_section) >= min_section_length:
                sections.append(current_section.strip())
                current_section = ""

        # æ·»åŠ æœ€åä¸€æ®µ
        if current_section:
            sections.append(current_section.strip())

        return sections

    def generate_qa_from_section(self, section: str, section_id: int) -> List[Dict]:
        """ä»æ–‡æœ¬æ®µè½ç”Ÿæˆé—®ç­”å¯¹"""
        qa_pairs = []

        if len(section) < 50:  # å¤ªçŸ­çš„æ®µè½è·³è¿‡
            return qa_pairs

        # 1. æ€»ç»“æ€§é—®é¢˜
        summary_q = f"è¯·æ€»ç»“è¿™ä¸€æ®µå†…å®¹"
        summary_a = f"è¿™ä¸€æ®µçš„ä¸»è¦å†…å®¹æ˜¯ï¼š{section[:200]}..."
        qa_pairs.append({
            "instruction": summary_q,
            "response": summary_a,
            "type": "summary",
            "source": f"section_{section_id}"
        })

        # 2. æå–å…³é”®ä¿¡æ¯ï¼ˆå¥å­çº§åˆ«ï¼‰
        sentences = [s.strip() for s in section.split('ã€‚') if s.strip()]
        for i, sentence in enumerate(sentences[:3]):  # å–å‰3ä¸ªå¥å­
            if len(sentence) > 20:
                detail_q = f"å…³äº'{sentence[:30]}...'çš„å…·ä½“å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ"
                detail_a = sentence
                qa_pairs.append({
                    "instruction": detail_q,
                    "response": detail_a,
                    "type": "detail",
                    "source": f"section_{section_id}_sentence_{i}"
                })

        # 3. æœ¯è¯­è§£é‡Š
        # æå–2-4å­—çš„ä¸­æ–‡è¯è¯­ä½œä¸ºå¯èƒ½æœ¯è¯­
        chinese_words = re.findall(r'[\u4e00-\u9fa5]{2,4}', section)
        for word in list(set(chinese_words))[:3]:  # å–å‰3ä¸ªä¸é‡å¤çš„è¯è¯­
            # æ‰¾åˆ°åŒ…å«è¿™ä¸ªè¯çš„ä¸Šä¸‹æ–‡
            context_sentences = [s for s in sentences if word in s]
            if context_sentences:
                term_q = f"ä»€ä¹ˆæ˜¯'{word}'ï¼Ÿ"
                term_a = context_sentences[0]
                qa_pairs.append({
                    "instruction": term_q,
                    "response": term_a,
                    "type": "term",
                    "source": f"section_{section_id}_term_{word}"
                })

        return qa_pairs

    def process_documents(self, generate_qa: bool = True) -> str:
        """å¤„ç†æ–‡æ¡£å¹¶ç”Ÿæˆè®­ç»ƒæ•°æ®"""
        print("=" * 60)
        print("ğŸ“š æ–‡æ¡£å¤„ç†ç³»ç»Ÿ")
        print("=" * 60)

        # æ£€æŸ¥æ–‡æ¡£ç›®å½•
        if not self.documents_dir.exists():
            print(f"ğŸ“ åˆ›å»ºæ–‡æ¡£ç›®å½•: {self.documents_dir}")
            self.documents_dir.mkdir(parents=True)

        # æŸ¥æ‰¾æ–‡æ¡£æ–‡ä»¶
        supported_extensions = ['.txt', '.pdf', '.docx', '.doc']
        doc_files = []
        for ext in supported_extensions:
            doc_files.extend(list(self.documents_dir.glob(f"*{ext}")))

        if not doc_files:
            print("âš ï¸  æœªæ‰¾åˆ°æ–‡æ¡£æ–‡ä»¶")
            print(f"ğŸ’¡ è¯·å°†æ–‡æ¡£æ”¾å…¥: {self.documents_dir}")
            print(f"ğŸ“„ æ”¯æŒæ ¼å¼: {', '.join(supported_extensions)}")
            return None

        print(f"ğŸ“ æ‰¾åˆ° {len(doc_files)} ä¸ªæ–‡æ¡£:")
        for f in doc_files:
            print(f"  â€¢ {f.name}")

        all_qa_pairs = []
        all_sections = []

        # å¤„ç†æ¯ä¸ªæ–‡æ¡£
        for doc_file in doc_files:
            print(f"\nğŸ“„ å¤„ç†: {doc_file.name}")

            # æå–æ–‡æœ¬
            text = self.extract_text_from_file(doc_file)
            if not text:
                print(f"  âš ï¸  æ— æ³•æå–æ–‡æœ¬ï¼Œè·³è¿‡")
                continue

            # æ¸…ç†æ–‡æœ¬
            cleaned_text = self.clean_text(text)
            print(f"  ğŸ“ åŸå§‹å­—ç¬¦: {len(text):,} â†’ æ¸…ç†å: {len(cleaned_text):,}")

            if not cleaned_text:
                print(f"  âš ï¸  æ¸…ç†åæ— å†…å®¹ï¼Œè·³è¿‡")
                continue

            # åˆ†å‰²æˆæ®µè½
            sections = self.split_into_sections(cleaned_text)
            print(f"  ğŸ“Š åˆ†å‰²æˆ {len(sections)} ä¸ªæ®µè½")

            all_sections.extend(sections)

            # ç”Ÿæˆé—®ç­”å¯¹
            if generate_qa:
                for i, section in enumerate(sections):
                    qa_pairs = self.generate_qa_from_section(section, i)
                    all_qa_pairs.extend(qa_pairs)

        # ä¿å­˜ç»“æœ
        if generate_qa and all_qa_pairs:
            # ä¿å­˜è®­ç»ƒæ•°æ®
            training_file = self.training_dir / "document_qa_data.jsonl"
            with open(training_file, 'w', encoding='utf-8') as f:
                for qa in all_qa_pairs:
                    f.write(json.dumps(qa, ensure_ascii=False) + '\n')

            print(f"\nâœ… å¤„ç†å®Œæˆï¼")
            print(f"ğŸ“Š æ–‡æ¡£æ®µè½: {len(all_sections)} ä¸ª")
            print(f"ğŸ“Š ç”Ÿæˆé—®ç­”: {len(all_qa_pairs)} å¯¹")
            print(f"ğŸ“ è®­ç»ƒæ•°æ®: {training_file}")

            return str(training_file)

        elif all_sections:
            # åªä¿å­˜æ–‡æœ¬
            text_file = self.training_dir / "document_texts.txt"
            with open(text_file, 'w', encoding='utf-8') as f:
                for i, section in enumerate(all_sections):
                    f.write(f"=== æ®µè½ {i + 1} ===\n")
                    f.write(section + "\n\n")

            print(f"\nâœ… æ–‡æœ¬æå–å®Œæˆ")
            print(f"ğŸ“Š æ–‡æ¡£æ®µè½: {len(all_sections)} ä¸ª")
            print(f"ğŸ“ æ–‡æœ¬æ–‡ä»¶: {text_file}")

            return str(text_file)

        return None

    def quick_start(self):
        """å¿«é€Ÿå¯åŠ¨æ–‡æ¡£å¤„ç†"""
        print("ğŸ¯ å¿«é€Ÿå¯åŠ¨æ–‡æ¡£è®­ç»ƒ")

        # 1. æ£€æŸ¥ä¾èµ–
        try:
            import transformers
            import torch
            print("âœ… æ ¸å¿ƒä¾èµ–å·²å®‰è£…")
        except ImportError:
            print("âŒ è¯·å…ˆå®‰è£…æ ¸å¿ƒä¾èµ–")
            print("ğŸ’¡ è¿è¡Œ: pip install transformers torch")
            return

        # 2. å¤„ç†æ–‡æ¡£
        training_file = self.process_documents(generate_qa=True)

        if training_file:
            print("\nğŸš€ ä¸‹ä¸€æ­¥:")
            print(f"è¿è¡Œæ–‡æ¡£è®­ç»ƒ: python -m src.document_qa.trainer --data {training_file}")
            print("\nğŸ’¡ æˆ–ä½¿ç”¨ç°æœ‰è®­ç»ƒç³»ç»Ÿ:")
            print(f"python train.py  # é€‰æ‹©æ–‡ä»¶: {training_file}")

        return training_file