"""
config.py - é¡¹ç›®é…ç½®æ–‡ä»¶
"""

import os
import sys
from pathlib import Path


# ==================== æ ¹ç›®å½•æ£€æµ‹ ====================
def get_project_root():
    """
    è‡ªåŠ¨æ£€æµ‹é¡¹ç›®æ ¹ç›®å½•
    æŒ‰ä¼˜å…ˆçº§å°è¯•å¤šç§æ–¹å¼ï¼š
    1. å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„çˆ¶çº§ï¼ˆå¦‚æœæ˜¯æ¨¡å—åŒ–ç»“æ„ï¼‰
    2. ç¯å¢ƒå˜é‡æŒ‡å®šçš„ç›®å½•
    3. å½“å‰å·¥ä½œç›®å½•
    """
    # æ–¹æ³•1ï¼šåŸºäº__file__çš„è·¯å¾„ï¼ˆæœ€å¯é ï¼‰
    try:
        # è·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„ï¼ˆconfig.pyæ‰€åœ¨ä½ç½®ï¼‰
        current_file = Path(__file__).resolve()



        # å¦‚æœconfig.pyåœ¨webç›®å½•ä¸‹ï¼Œè¿”å›ä¸Šä¸€çº§
        if current_file.parent.name == "web":
            return current_file.parent.parent.parent
            # å¦‚æœconfig.pyåœ¨srcç›®å½•ä¸‹ï¼Œè¿”å›ä¸Šä¸€çº§
        elif current_file.parent.name == "src":
            return current_file.parent.parent

        # å…¶ä»–æƒ…å†µï¼Œå‘ä¸Šæ‰¾åŒ…å«README.mdçš„ç›®å½•ä½œä¸ºæ ¹ç›®å½•
        for parent in current_file.parents:
            if (parent / "README.md").exists() or (parent / "requirements.txt").exists():
                return parent
    except Exception as e:
        print(f"âš ï¸  åŸºäº__file__æ£€æµ‹æ ¹ç›®å½•å¤±è´¥: {e}")

    # æ–¹æ³•2ï¼šç¯å¢ƒå˜é‡
    project_root_env = os.environ.get("AMD_AI_PROJECT_ROOT")
    if project_root_env:
        env_root = Path(project_root_env).resolve()
        if env_root.exists():
            return env_root

    # æ–¹æ³•3ï¼šå½“å‰å·¥ä½œç›®å½•
    cwd = Path.cwd()
    if (cwd / "README.md").exists() or (cwd / "requirements.txt").exists():
        return cwd

    # æ–¹æ³•4ï¼šè„šæœ¬è¿è¡Œç›®å½•
    if hasattr(sys, '_MEIPASS'):
        # PyInstalleræ‰“åŒ…åçš„ä¸´æ—¶ç›®å½•
        bundle_dir = Path(sys._MEIPASS)
        if (bundle_dir / "README.md").exists():
            return bundle_dir

    # æœ€åå°è¯•ï¼šå‡è®¾å½“å‰ç›®å½•æ˜¯æ ¹ç›®å½•
    return Path.cwd()


# å…¨å±€å˜é‡ - é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = get_project_root()
MODELS_DIR = PROJECT_ROOT / "models"
CACHE_DIR = PROJECT_ROOT / ".cache"
DATA_DIR = PROJECT_ROOT / "data"
LOGS_DIR = PROJECT_ROOT / "logs"

# ç¡®ä¿ç›®å½•å­˜åœ¨
for directory in [MODELS_DIR, CACHE_DIR, DATA_DIR, LOGS_DIR]:
    directory.mkdir(exist_ok=True, parents=True)

# ==================== æ¨¡å‹é…ç½® ====================
# æ¨¡å‹åŸºæœ¬é…ç½®æ¨¡æ¿
MODEL_CONFIG_TEMPLATE = {
    "description": "",
    "size_gb": 0,
    "recommended_vram": 0,
    "cache_dir": None,
    "type": "unknown"
}

# HuggingFaceæ¨¡å‹é…ç½®
HF_MODELS = {
    "Qwen2.5-0.5B": {
        "name": "Qwen/Qwen2.5-0.5B-Instruct",
        "description": "é€Ÿåº¦å¿«ï¼Œé€‚åˆèŠå¤©",
        "size_gb": 1.0,
        "recommended_vram": 4,
        "cache_dir": str(CACHE_DIR / "huggingface"),
        "type": "huggingface"
    },
    "Qwen2.5-1.5B": {
        "name": "Qwen/Qwen2.5-1.5B-Instruct",
        "description": "å¹³è¡¡æ€§èƒ½ï¼Œæ›´èªæ˜",
        "size_gb": 3.0,
        "recommended_vram": 8,
        "cache_dir": str(CACHE_DIR / "huggingface"),
        "type": "huggingface"
    },
    "Qwen2.5-3B": {
        "name": "Qwen/Qwen2.5-3B-Instruct",
        "description": "èƒ½åŠ›å¼ºï¼Œå›ç­”è¯¦ç»†",
        "size_gb": 6.0,
        "recommended_vram": 12,
        "cache_dir": str(CACHE_DIR / "huggingface"),
        "type": "huggingface"
    }
}

# æœ¬åœ°æ¨¡å‹è·¯å¾„æ˜ å°„ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
LOCAL_MODEL_PATHS = {
    "Qwen3-0.6B": MODELS_DIR / "qianwen3" / "qianwen0.6",
    "Qwen2.5-0.5B-æ–‡æ¡£ç‰ˆ": MODELS_DIR / "trained" / "20260129_195952" / "final_model",
    "Qwen2.5-0.5B-é›ªé›ªè®­ç»ƒ": MODELS_DIR / "trained" / "20260201_214732" / "checkpoint-400",
    "Qwen2.5-0.5B-é˜¿ç±³å¨…è®­ç»ƒ": MODELS_DIR / "trained" / "20260202_170728" / "final_model"
}

# ==================== ç”Ÿæˆé…ç½® ====================
GENERATION_CONFIG = {
    "default": {
        "max_new_tokens": 512,
        "temperature": 0.7,
        "top_p": 0.8,
        "repetition_penalty": 1.05,
        "do_sample": True,
        "pad_token_id": None,  # è‡ªåŠ¨è®¾ç½®
        "eos_token_id": None  # è‡ªåŠ¨è®¾ç½®
    },
    "creative": {
        "max_new_tokens": 768,
        "temperature": 0.8,
        "top_p": 0.9,
        "repetition_penalty": 1.1,
        "do_sample": True
    },
    "technical": {
        "max_new_tokens": 384,
        "temperature": 0.3,
        "top_p": 0.95,
        "repetition_penalty": 1.02,
        "do_sample": False
    }
}

# ==================== ç•Œé¢é…ç½® ====================
UI_CONFIG = {
    "server_port": 7860,
    "server_name": "0.0.0.0",
    "share": False,
    "theme": "soft",
    "height": 550,
    "chatbot_height": 550,
    "memory_file": str(DATA_DIR / "conversation_memory.pkl"),
    "max_memory_items": 10
}

# ==================== æ—¥å¿—é…ç½® ====================
LOG_CONFIG = {
    "level": "INFO",
    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    "file": str(LOGS_DIR / "ai_assistant.log"),
    "max_size_mb": 10,
    "backup_count": 5
}

# ==================== å¯¼å‡ºæ‰€æœ‰é…ç½® ====================
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ“ é¡¹ç›®é…ç½®ä¿¡æ¯")
    print("=" * 60)
    print(f"é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}")
    print(f"æ¨¡å‹ç›®å½•: {MODELS_DIR}")
    print(f"ç¼“å­˜ç›®å½•: {CACHE_DIR}")
    print(f"æ•°æ®ç›®å½•: {DATA_DIR}")
    print(f"æ—¥å¿—ç›®å½•: {LOGS_DIR}")
    print("=" * 60)
    print(f"å¯ç”¨çš„HuggingFaceæ¨¡å‹: {list(HF_MODELS.keys())}")
    print(f"å¯ç”¨çš„æœ¬åœ°æ¨¡å‹è·¯å¾„: {list(LOCAL_MODEL_PATHS.keys())}")
    print("=" * 60)