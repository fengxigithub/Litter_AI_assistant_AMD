"""
AMD 7900XTX AIåŠ©æ‰‹ - å®Œæ•´åŠŸèƒ½ç‰ˆ
æ”¯æŒæ¨¡å‹åˆ‡æ¢ + å¯¹è¯è®°å¿†
"""
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import (
    PROJECT_ROOT, MODELS_DIR, CACHE_DIR, DATA_DIR,
    LOCAL_MODEL_PATHS,HF_MODELS, UI_CONFIG
)

print("=" * 60)
print("ğŸš€ AMD 7900XTX AIåŠ©æ‰‹ - å®Œæ•´åŠŸèƒ½ç‰ˆ")
print("=" * 60)
print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}")
print(f"ğŸ“ æ¨¡å‹ç›®å½•: {MODELS_DIR}")
print(f"ğŸ“ ç¼“å­˜ç›®å½•: {CACHE_DIR}")
print(f"ğŸ“ æ•°æ®ç›®å½•: {DATA_DIR}")
print("=" * 60)

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["TRANSFORMERS_CACHE"] = str(CACHE_DIR / "huggingface")

import gradio as gr
import torch
import webbrowser
import json
import time
import pickle
from pathlib import Path
from datetime import datetime

# æ£€æŸ¥DirectML
try:
    import torch_directml
    DML_AVAILABLE = True
    print("âœ… DirectMLå¯ç”¨")
except ImportError:
    DML_AVAILABLE = False
    print("âš ï¸  DirectMLæœªå®‰è£…")

print(f"ğŸ”§ PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"ğŸ”§ Gradioç‰ˆæœ¬: {gr.__version__}")
print()

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”è®°å¿†ç®¡ç†å™¨
class MemoryManager:
    """å¯¹è¯è®°å¿†ç®¡ç†å™¨"""

    def __init__(self, memory_file=UI_CONFIG["memory_file"], max_memory=UI_CONFIG["max_memory_items"]):
        self.memory_file = Path(memory_file)
        self.max_memory = max_memory
        self.conversations = self.load_memory()

        print(f"ğŸ“ è®°å¿†æ–‡ä»¶: {self.memory_file}")
        print(f"ğŸ“ æœ€å¤§è®°å¿†æ¡æ•°: {self.max_memory}")

    def load_memory(self):
        """åŠ è½½å¯¹è¯è®°å¿†"""
        if self.memory_file.exists():
            try:
                with open(self.memory_file, 'rb') as f:
                    data = pickle.load(f)
                print(f"âœ… åŠ è½½å¯¹è¯è®°å¿†: {len(data)} æ¡è®°å½•")
                return data
            except Exception as e:
                print(f"âš ï¸  è®°å¿†åŠ è½½å¤±è´¥: {e}")
        return []

    def save_memory(self):
        """ä¿å­˜å¯¹è¯è®°å¿†"""
        try:
            if len(self.conversations) > self.max_memory:
                self.conversations = self.conversations[-self.max_memory:]

            with open(self.memory_file, 'wb') as f:
                pickle.dump(self.conversations, f)
            return True
        except Exception as e:
            print(f"âŒ è®°å¿†ä¿å­˜å¤±è´¥: {e}")
            return False

    def add_conversation(self, user_message, ai_response, model_used):
        """æ·»åŠ å¯¹è¯è®°å½•"""
        conversation = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "user": user_message,
            "ai": ai_response,
            "model": model_used,
            "tokens": len(ai_response.split())
        }

        self.conversations.append(conversation)
        self.save_memory()

        return self.format_conversation(conversation)

    def format_conversation(self, conv):
        """æ ¼å¼åŒ–å•æ¡å¯¹è¯è®°å½•"""
        return f"[{conv['timestamp']}] {conv['model']}\nğŸ‘¤ {conv['user'][:50]}...\nğŸ¤– {conv['ai'][:100]}...\n"

    def get_recent_memory(self, count=5):
        """è·å–æœ€è¿‘çš„å¯¹è¯è®°å¿†"""
        recent = self.conversations[-count:] if self.conversations else []
        if recent:
            return "ğŸ“š æœ€è¿‘å¯¹è¯:\n" + "\n".join([self.format_conversation(c) for c in recent])
        return "ğŸ“š æš‚æ— å¯¹è¯å†å²"

    def clear_memory(self):
        """æ¸…ç©ºå¯¹è¯è®°å¿†"""
        self.conversations = []
        if self.memory_file.exists():
            self.memory_file.unlink()
        print("âœ… å¯¹è¯è®°å¿†å·²æ¸…ç©º")
        return "ğŸ—‘ï¸ å¯¹è¯è®°å¿†å·²æ¸…ç©º"

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”ModelManagerç±»
class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨"""

    def __init__(self):
        # ä½¿ç”¨å…¨å±€é…ç½®
        self.project_root = PROJECT_ROOT
        self.models_dir = MODELS_DIR

        # å¯ç”¨æ¨¡å‹é…ç½®ï¼ˆåŠ¨æ€åˆå¹¶ï¼‰
        self.available_models = self._init_models()

        self.current_model = None
        self.current_model_key = "Qwen2.5-0.5B"  # é»˜è®¤æ¨¡å‹
        self.device = None
        self.model = None
        self.tokenizer = None
        self.model_loaded = False

        print(f"ğŸ“Š å¯ç”¨çš„æ¨¡å‹:")
        for model_key, info in self.available_models.items():
            status = "âœ…" if info.get("type") != "local" or self._check_model_exists(info["name"]) else "âš ï¸ "
            print(f"  {status} {model_key} ({info['type']})")

    def _init_models(self):
        """åˆå§‹åŒ–æ¨¡å‹é…ç½®ï¼ˆåˆå¹¶HFå’Œæœ¬åœ°æ¨¡å‹ï¼‰"""
        models = {}



        # 2. æ·»åŠ æœ¬åœ°æ¨¡å‹ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
        for model_key, model_path in LOCAL_MODEL_PATHS.items():
            if model_path.exists():
                models[model_key] = {
                    "name": str(model_path),
                    "description": f"æœ¬åœ°æ¨¡å‹: {model_key}",
                    "size_gb": 1.0,  # å¯ä»¥æ ¹æ®å®é™…æ–‡ä»¶å¤§å°è°ƒæ•´
                    "recommended_vram": 4,
                    "cache_dir": None,
                    "type": "local"
                }
            else:
                print(f"âš ï¸  æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")

        # 1. æ·»åŠ HuggingFaceæ¨¡å‹
        for key, config in HF_MODELS.items():
            models[key] = config.copy()  # æ·±æ‹·è´ï¼Œé¿å…ä¿®æ”¹åŸå§‹é…ç½®

        return models

    def _check_model_exists(self, model_path_str):
        """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        model_path = Path(model_path_str)

        # å¦‚æœæ˜¯HuggingFaceæ¨¡å‹åï¼Œç›´æ¥è¿”å›Trueï¼ˆä¼šåœ¨çº¿ä¸‹è½½ï¼‰
        if "/" in model_path_str and not model_path.exists():
            return True

        if not model_path.exists():
            return False

        # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„æ¨¡å‹æ–‡ä»¶
        required_patterns = [
            "*.bin", "*.safetensors",  # æ¨¡å‹æƒé‡
            "config.json", "*.json",  # é…ç½®æ–‡ä»¶
            "tokenizer.json", "*.model"  # tokenizeræ–‡ä»¶
        ]

        for pattern in required_patterns:
            if list(model_path.glob(pattern)):
                return True

        return False

    def get_model_info(self, model_key):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if model_key in self.available_models:
            info = self.available_models[model_key]

            # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
            status = "âœ… å¯ç”¨"
            if info.get("type") == "local":
                if not self._check_model_exists(info["name"]):
                    status = "âŒ æ–‡ä»¶ä¸å­˜åœ¨"

            model_path = Path(info["name"]) if info.get("type") == "local" else info["name"]

            return (
                f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:\n"
                f"â€¢ åç§°: {model_key}\n"
                f"â€¢ è·¯å¾„: {model_path}\n"
                f"â€¢ çŠ¶æ€: {status}\n"
                f"â€¢ æè¿°: {info['description']}\n"
                f"â€¢ å¤§å°: {info['size_gb']} GB\n"
                f"â€¢ æ¨èæ˜¾å­˜: {info['recommended_vram']} GB\n"
                f"â€¢ ç±»å‹: {info.get('type', 'æœªçŸ¥')}"
            )
        return "âŒ æœªçŸ¥æ¨¡å‹"

    def setup_device(self):
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if DML_AVAILABLE:
            import torch_directml
            self.device = torch_directml.device()
            return f"ğŸ® DirectMLè®¾å¤‡: {self.device}"
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
            return "ğŸ® CUDAè®¾å¤‡"
        else:
            self.device = torch.device("cpu")
            return "âš ï¸  CPUè®¾å¤‡"

    def check_vram_sufficient(self, model_key):
        """æ£€æŸ¥æ˜¾å­˜æ˜¯å¦è¶³å¤Ÿ"""
        if model_key not in self.available_models:
            return False, "æœªçŸ¥æ¨¡å‹"

        model_info = self.available_models[model_key]
        required_vram = model_info["recommended_vram"]

        if DML_AVAILABLE:
            try:
                import torch_directml
                has_enough = True
                message = f"âœ… AMD 7900XTX 24GBæ˜¾å­˜ï¼Œå¯è¿è¡Œ{model_key}"
            except:
                has_enough = True
                message = f"âš ï¸  æ— æ³•æ£€æµ‹DirectMLæ˜¾å­˜ï¼Œå‡è®¾è¶³å¤Ÿè¿è¡Œ{model_key}"
        elif torch.cuda.is_available():
            total_vram = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
            has_enough = total_vram >= required_vram
            message = f"GPUæ˜¾å­˜: {total_vram:.1f}GB / éœ€è¦: {required_vram}GB"
        else:
            has_enough = False
            message = "âŒ æ— GPUå¯ç”¨ï¼Œåªèƒ½ä½¿ç”¨CPUè¿è¡Œå°æ¨¡å‹"

        return has_enough, message

    def load_model(self, model_key):
        """åŠ è½½æŒ‡å®šæ¨¡å‹"""
        if model_key not in self.available_models:
            return "âŒ æ— æ•ˆçš„æ¨¡å‹é€‰æ‹©"

        if self.model_loaded and model_key == self.current_model_key:
            return "âœ… æ¨¡å‹å·²åŠ è½½ï¼Œæ— éœ€é‡æ–°åŠ è½½"

        print("=" * 50)
        print(f"ğŸ”„ åˆ‡æ¢æ¨¡å‹åˆ°: {model_key}")
        print("=" * 50)

        # æ£€æŸ¥æ˜¾å­˜
        has_enough, vram_msg = self.check_vram_sufficient(model_key)
        if not has_enough:
            return f"âŒ æ˜¾å­˜ä¸è¶³: {vram_msg}"

        model_info = self.available_models[model_key]

        # æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ–‡ä»¶
        if model_info.get("type") == "local":
            model_path = Path(model_info["name"])
            if not model_path.exists():
                return (
                    f"âŒ æœ¬åœ°æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨\n"
                    f"è·¯å¾„: {model_path}\n\n"
                    f"ğŸ’¡ è¯·å°†æ¨¡å‹æ–‡ä»¶å¤åˆ¶åˆ°ä¸Šè¿°ä½ç½®\n"
                    f"æˆ–é€‰æ‹©å…¶ä»–åœ¨çº¿æ¨¡å‹"
                )

        try:
            # è®¾ç½®è®¾å¤‡
            device_msg = self.setup_device()
            print(device_msg)

            # å¦‚æœå·²æœ‰æ¨¡å‹ï¼Œå…ˆæ¸…ç†
            if self.model is not None:
                del self.model
                del self.tokenizer
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                print("ğŸ§¹ æ¸…ç†ä¸Šä¸€ä¸ªæ¨¡å‹")

            # å¯¼å…¥transformers
            from transformers import AutoTokenizer, AutoModelForCausalLM

            # åŠ è½½tokenizer
            print(f"ğŸ”§ åŠ è½½tokenizer: {model_info['name']}")

            # è®¾ç½®ç¼“å­˜ç›®å½•
            cache_dir = model_info.get("cache_dir")
            if cache_dir and model_info.get("type") == "huggingface":
                print(f"ğŸ“ ä½¿ç”¨ç¼“å­˜ç›®å½•: {cache_dir}")

            self.tokenizer = AutoTokenizer.from_pretrained(
                model_info["name"],
                cache_dir=cache_dir,
                trust_remote_code=False,
                local_files_only=(model_info.get("type") == "local")
            )

            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            print(f"âœ… TokenizeråŠ è½½æˆåŠŸ (è¯æ±‡é‡: {self.tokenizer.vocab_size:,})")

            # åŠ è½½æ¨¡å‹
            print("ğŸ”§ åŠ è½½æ¨¡å‹...")
            dtype = torch.float16 if self.device.type != "cpu" else torch.float32

            self.model = AutoModelForCausalLM.from_pretrained(
                model_info["name"],
                torch_dtype=dtype,
                cache_dir=cache_dir,
                trust_remote_code=True,
                local_files_only=(model_info.get("type") == "local")
            ).to(self.device)

            self.model.eval()
            self.model_loaded = True
            self.current_model_key = model_key
            self.current_model = model_info["name"]

            # é¢„çƒ­æ¨¡å‹
            print("ğŸ”¥ æ¨¡å‹é¢„çƒ­...")
            self._warmup_model()

            print("=" * 50)
            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

            model_name_display = model_key
            device_type = "DirectML" if DML_AVAILABLE else "CPU"
            if torch.cuda.is_available():
                device_type = "CUDA"

            message = [
                f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼",
                f"ğŸ¤– å½“å‰æ¨¡å‹: {model_name_display}",
                f"ğŸ® è¿è¡Œè®¾å¤‡: {device_type}",
                f"ğŸ“Š è¯æ±‡é‡: {self.tokenizer.vocab_size:,}",
                f"ğŸ’¾ æ¨¡å‹å¤§å°: {model_info['size_gb']} GB",
                f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_info['name']}",
                f"",
                f"ğŸ’¡ å¯ä»¥å¼€å§‹èŠå¤©äº†ï¼"
            ]

            return "\n".join(message)

        except Exception as e:
            error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            return error_msg

    def _warmup_model(self):
        """é¢„çƒ­æ¨¡å‹"""
        try:
            warmup_text = "ä½ å¥½"
            inputs = self.tokenizer(warmup_text, return_tensors="pt").to(self.device)
            with torch.no_grad():
                _ = self.model.generate(
                    **inputs,
                    max_new_tokens=10,
                    pad_token_id=self.tokenizer.pad_token_id,
                )
            print("âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸  é¢„çƒ­å¤±è´¥: {e}")

    def generate_response(self, message, history=None):
        """ç”Ÿæˆå›å¤ï¼ˆåŒ…å«å†å²ä¸Šä¸‹æ–‡ï¼‰"""
        if not self.model_loaded:
            return "è¯·å…ˆåŠ è½½æ¨¡å‹ï¼", None

        try:
            start_time = time.time()

            messages = []

            if history and len(history) > 0:
                for msg in history[-8:]:
                    if isinstance(msg, dict) and "role" in msg and "content" in msg:
                        messages.append({
                            "role": msg["role"],
                            "content": str(msg["content"])
                        })

            messages.append({"role": "user", "content": message})

            print(f"ğŸ“ å‘é€ç»™æ¨¡å‹çš„æ¶ˆæ¯æ ¼å¼: {messages}")

            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            inputs = self.tokenizer(text, return_tensors="pt").to(self.device)

            max_tokens = 512
            if "1.5B" in self.current_model_key:
                max_tokens = 384
            elif "3B" in self.current_model_key:
                max_tokens = 512

            input_length = inputs["input_ids"].shape[1]
            max_tokens = min(1024 - input_length, max_tokens)

            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=0.7,
                    top_p=0.8,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.05,
                )

            response = self.tokenizer.decode(
                outputs[0][inputs["input_ids"].shape[1]:],
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )

            gen_time = time.time() - start_time
            tokens = outputs.shape[1] - inputs["input_ids"].shape[1]
            speed = tokens / gen_time if gen_time > 0 else 0

            output = f"{response}\n\n{'=' * 40}\n"
            output += f"ğŸ“Š ç”Ÿæˆç»Ÿè®¡:\n"
            output += f"â€¢ é€Ÿåº¦: {speed:.1f} token/ç§’\n"
            output += f"â€¢ é•¿åº¦: {tokens} tokens\n"
            output += f"â€¢ æ—¶é—´: {gen_time:.2f}ç§’\n"
            output += f"â€¢ ä¸Šä¸‹æ–‡é•¿åº¦: {input_length} tokens\n"
            output += f"â€¢ æ¨¡å‹: {self.current_model_key}\n"
            output += f"â€¢ ç±»å‹: {self.available_models[self.current_model_key].get('type', 'æœªçŸ¥')}\n"
            output += f"{'=' * 40}"

            return output, self.current_model_key

        except Exception as e:
            error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            return error_msg, None


# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”EnhancedAIChatç±»
class EnhancedAIChat:
    """å¢å¼ºç‰ˆAIèŠå¤©"""

    def __init__(self):
        self.model_manager = ModelManager()
        self.memory_manager = MemoryManager()
        self.qa_process = None
        self.qa_port = 7861

    def get_model_list(self):
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        models = list(self.model_manager.available_models.keys())
        # æŒ‰ç±»å‹æ’åºï¼šæœ¬åœ°æ¨¡å‹åœ¨å‰ï¼Œåœ¨çº¿æ¨¡å‹åœ¨å
        local_models = [m for m in models if self.model_manager.available_models[m].get("type") == "local"]
        online_models = [m for m in models if self.model_manager.available_models[m].get("type") == "huggingface"]
        return local_models + online_models

    def get_model_details(self, model_key):
        """è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯"""
        return self.model_manager.get_model_info(model_key)

    def switch_model(self, model_key):
        """åˆ‡æ¢æ¨¡å‹"""
        return self.model_manager.load_model(model_key)

    def chat(self, message, history):
        """å¤„ç†èŠå¤©"""
        text_history = []

        if history:
            for msg in history:
                if isinstance(msg, dict):
                    role = msg.get("role", "")
                    content = msg.get("content", "")

                    if isinstance(content, list):
                        text_content = ""
                        for item in content:
                            if isinstance(item, dict):
                                if item.get("type") == "text":
                                    text_content += item.get("text", "")
                            elif isinstance(item, str):
                                text_content += item

                        if text_content and role in ["user", "assistant"]:
                            text_history.append({"role": role, "content": text_content})
                    elif isinstance(content, str):
                        text_history.append({"role": role, "content": content})

        response, model_used = self.model_manager.generate_response(message, text_history)

        if model_used and "âŒ" not in response:
            self.memory_manager.add_conversation(message, response, model_used)

        return response, model_used

    def get_memory_summary(self):
        """è·å–è®°å¿†æ‘˜è¦"""
        return self.memory_manager.get_recent_memory(5)

    def clear_memory(self):
        """æ¸…ç©ºè®°å¿†"""
        return self.memory_manager.clear_memory()

    def get_current_model(self):
        """è·å–å½“å‰æ¨¡å‹"""
        return self.model_manager.current_model_key

    def launch_qa_interface(self):
        """å¯åŠ¨QAç•Œé¢å¹¶è¿”å›URL"""
        import subprocess
        import sys
        import os
        from pathlib import Path

        if self.qa_process is not None:
            try:
                self.qa_process.terminate()
                self.qa_process = None
            except:
                pass

        try:
            base_dir = Path(__file__).parent.parent.parent
            qa_script_path = base_dir / "src" / "qa" / "qa_interface.py"

            if not qa_script_path.exists():
                return f"âŒ QAè„šæœ¬ä¸å­˜åœ¨: {qa_script_path}"

            print(f"ğŸš€ å¯åŠ¨QAç•Œé¢: {qa_script_path}")
            print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {base_dir}")

            env = os.environ.copy()
            python_path = env.get('PYTHONPATH', '')
            if str(base_dir) not in python_path:
                env['PYTHONPATH'] = f"{str(base_dir)}{os.pathsep}{python_path}"
            env['HF_ENDPOINT'] = "https://hf-mirror.com"

            self.qa_process = subprocess.Popen(
                [sys.executable, str(qa_script_path)],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                cwd=str(base_dir),
                env=env,
                creationflags=subprocess.CREATE_NEW_PROCESS_GROUP if sys.platform == "win32" else 0
            )

            import time
            time.sleep(3)

            if self.qa_process.poll() is not None:
                stdout, stderr = self.qa_process.communicate()
                error_msg = f"âŒ QAè¿›ç¨‹å·²é€€å‡º:\næ ‡å‡†è¾“å‡º:\n{stdout}\n\næ ‡å‡†é”™è¯¯:\n{stderr}"
                print(error_msg)
                return error_msg

            qa_url = f"http://127.0.0.1:{self.qa_port}"
            qa_local_url = f"http://localhost:{self.qa_port}"

            return (f"âœ… QAç•Œé¢å·²å¯åŠ¨ï¼\n"
                    f"ğŸ“ å·¥ä½œç›®å½•: {base_dir}\n"
                    f"ğŸ”— æœ¬åœ°è®¿é—®: {qa_local_url}\n"
                    f"ğŸŒ ç½‘ç»œè®¿é—®: {qa_url}\n\n"
                    f"ğŸ’¡ ç‚¹å‡»é“¾æ¥æˆ–åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ä¸Šè¿°åœ°å€")

        except Exception as e:
            import traceback
            error_msg = f"âŒ å¯åŠ¨QAç•Œé¢å¤±è´¥: {str(e)}\n{traceback.format_exc()}"
            print(error_msg)
            return error_msg

    def stop_qa_interface(self):
        """åœæ­¢QAç•Œé¢"""
        if self.qa_process is not None:
            try:
                self.qa_process.terminate()
                self.qa_process = None
                return "âœ… QAç•Œé¢å·²åœæ­¢"
            except Exception as e:
                return f"âŒ åœæ­¢QAç•Œé¢å¤±è´¥: {e}"
        return "â„¹ï¸ æ²¡æœ‰è¿è¡Œçš„QAç•Œé¢"

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”ç•Œé¢å¸ƒå±€
def create_enhanced_interface():
    """åˆ›å»ºå¢å¼ºç‰ˆç•Œé¢"""
    ai_chat = EnhancedAIChat()

    print("ğŸ”„ åˆ›å»ºç•Œé¢ä¸­...")
    print(f"ğŸ“Š å¯ç”¨æ¨¡å‹: {ai_chat.get_model_list()}")

    with gr.Blocks(title="AMD 7900XTX AIåŠ©æ‰‹ - å¢å¼ºç‰ˆ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸš€ AMD 7900XTX AIåŠ©æ‰‹ - å¢å¼ºç‰ˆ")
        gr.Markdown("**æ¨¡å‹åˆ‡æ¢** | **å¯¹è¯è®°å¿†** | **DirectMLåŠ é€Ÿ**")
        gr.Markdown(f"ğŸ“ é¡¹ç›®ç›®å½•: `{PROJECT_ROOT}`")

        with gr.Row():
            # å·¦ä¾§æ§åˆ¶é¢æ¿
            with gr.Column(scale=1):
                gr.Markdown("### âš™ï¸ æ¨¡å‹æ§åˆ¶")

                model_selector = gr.Dropdown(
                    choices=ai_chat.get_model_list(),
                    value=ai_chat.get_model_list()[0] if ai_chat.get_model_list() else None,
                    label="é€‰æ‹©AIæ¨¡å‹",
                    interactive=True
                )

                model_info = gr.Textbox(
                    label="ğŸ“Š æ¨¡å‹ä¿¡æ¯",
                    value=ai_chat.get_model_details(
                        ai_chat.get_model_list()[0]) if ai_chat.get_model_list() else "æ— å¯ç”¨æ¨¡å‹",
                    lines=8,
                    interactive=False
                )

                load_btn = gr.Button("ğŸš€ åŠ è½½/åˆ‡æ¢æ¨¡å‹", variant="primary")

                status_display = gr.Textbox(
                    label="ğŸ“ˆ åŠ è½½çŠ¶æ€",
                    value="è¯·é€‰æ‹©æ¨¡å‹å¹¶ç‚¹å‡»åŠ è½½",
                    lines=6,
                    interactive=False
                )

                gr.Markdown("---")
                gr.Markdown("### ğŸ§  å¯¹è¯è®°å¿†")

                memory_display = gr.Textbox(
                    label="ğŸ“š è®°å¿†å†å²",
                    value=ai_chat.get_memory_summary(),
                    lines=8,
                    interactive=False
                )

                with gr.Row():
                    refresh_memory_btn = gr.Button("ğŸ”„ åˆ·æ–°è®°å¿†")
                    clear_memory_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºè®°å¿†", variant="stop")

                current_model_display = gr.Textbox(
                    label="ğŸ¤– å½“å‰æ¨¡å‹",
                    value="æœªåŠ è½½",
                    interactive=False
                )

                # QAé¢æ¿
                gr.Markdown("---")
                gr.Markdown("### ğŸ“„ æ–‡æ¡£é—®ç­” (QA)")

                qa_status = gr.Textbox(
                    label="QAçŠ¶æ€",
                    value="æœªå¯åŠ¨",
                    lines=4,
                    interactive=False
                )

                qa_url_display = gr.Textbox(
                    label="QAè®¿é—®åœ°å€",
                    value="ç‚¹å‡»å¯åŠ¨æŒ‰é’®è·å–åœ°å€",
                    lines=3,
                    interactive=False
                )

                with gr.Row():
                    qa_start_btn = gr.Button("ğŸš€ å¯åŠ¨QAç•Œé¢", variant="secondary")
                    qa_stop_btn = gr.Button("ğŸ›‘ åœæ­¢QA", variant="stop")

                with gr.Row():
                    qa_open_btn = gr.Button("ğŸŒ æµè§ˆå™¨æ‰“å¼€QAç•Œé¢", variant="primary")

                gr.Markdown("ğŸ’¡ **æç¤º**: QAç•Œé¢å¯åŠ¨åï¼Œè¯·å¤åˆ¶ä¸Šæ–¹åœ°å€åˆ°æµè§ˆå™¨æ‰“å¼€")

            # å³ä¾§èŠå¤©åŒºåŸŸ
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ’¬ èŠå¤©ç•Œé¢")

                chatbot = gr.Chatbot(
                    label="å¯¹è¯",
                    height=550,
                )

                with gr.Row():
                    msg = gr.Textbox(
                        label="ğŸ’­ è¾“å…¥æ¶ˆæ¯",
                        placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                        lines=3,
                        max_lines=5,
                        scale=4
                    )
                    deep_think_btn = gr.Button(
                        "ğŸ¤” æ·±åº¦æ€è€ƒ",
                        variant="secondary",
                        scale=1,
                        size="sm",
                        min_width=100
                    )
                    no_deep_think_btn = gr.Button(
                        "ğŸ¤” å¦æ·±åº¦æ€è€ƒ",
                        variant="secondary",
                        scale=1,
                        size="sm",
                        min_width=100
                    )

                with gr.Row():
                    send_btn = gr.Button("ğŸ“¤ å‘é€", variant="primary")
                    clear_chat_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå½“å‰å¯¹è¯")

                stats_display = gr.Textbox(
                    label="ğŸ“Š å®æ—¶ç»Ÿè®¡",
                    value="ç­‰å¾…ç¬¬ä¸€æ¬¡ç”Ÿæˆ...",
                    interactive=False
                )

        # ===== äº‹ä»¶ç»‘å®š =====
        # æ·±åº¦æ€è€ƒæŒ‰é’®
        def deep_think_prefix(message):
            if not message.strip().startswith('/think'):
                return message + "/think "
            return message

        deep_think_btn.click(
            fn=deep_think_prefix,
            inputs=[msg],
            outputs=[msg]
        )

        def no_deep_think_prefix(message):
            if not message.strip().startswith('/no_think'):
                return message + "/no_think "
            return message

        no_deep_think_btn.click(
            fn=no_deep_think_prefix,
            inputs=[msg],
            outputs=[msg]
        )

        # 1. æ¨¡å‹é€‰æ‹©å™¨æ›´æ–°ä¿¡æ¯
        def update_model_info(model_key):
            return ai_chat.get_model_details(model_key)

        model_selector.change(
            update_model_info,
            inputs=[model_selector],
            outputs=[model_info]
        )

        # 2. åŠ è½½/åˆ‡æ¢æ¨¡å‹
        def on_load_model(model_key):
            status = ai_chat.switch_model(model_key)
            current_model = ai_chat.get_current_model() or "æœªåŠ è½½"
            return status, current_model

        load_btn.click(
            on_load_model,
            inputs=[model_selector],
            outputs=[status_display, current_model_display]
        )

        # 3. å‘é€æ¶ˆæ¯
        def on_send_message(message, history, current_model):
            if not message.strip():
                return "", history, "è¯·è¾“å…¥æœ‰æ•ˆæ¶ˆæ¯", current_model

            if current_model == "æœªåŠ è½½":
                return "", history, "âš ï¸ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼", current_model

            try:
                processed_history = []
                if history:
                    for msg in history:
                        if isinstance(msg, dict):
                            role = msg.get("role", "")
                            content = msg.get("content", "")

                            if isinstance(content, list):
                                text_parts = []
                                for item in content:
                                    if isinstance(item, dict) and item.get("type") == "text":
                                        text_parts.append(item.get("text", ""))
                                    elif isinstance(item, str):
                                        text_parts.append(item)

                                if text_parts:
                                    processed_history.append({
                                        "role": role,
                                        "content": " ".join(text_parts)
                                    })
                            elif isinstance(content, str):
                                processed_history.append({"role": role, "content": content})

                response, model_used = ai_chat.chat(message, processed_history)

                if history is None:
                    history = []

                history.append({
                    "role": "user",
                    "content": [{"type": "text", "text": message}]
                })

                history.append({
                    "role": "assistant",
                    "content": [{"type": "text", "text": response}]
                })

                stats_text = "ç”Ÿæˆå®Œæˆ"
                if "ç”Ÿæˆç»Ÿè®¡:" in response:
                    try:
                        stats_lines = response.split("ç”Ÿæˆç»Ÿè®¡:")[1].split("\n")
                        stats = [line.strip() for line in stats_lines if line.strip().startswith("â€¢")]
                        if stats:
                            stats_text = " | ".join([s.replace("â€¢ ", "") for s in stats[:2]])
                    except:
                        pass

                return "", history, stats_text, current_model

            except Exception as e:
                error_msg = f"å‘é€å¤±è´¥: {str(e)}"
                print(f"âŒ é”™è¯¯: {error_msg}")
                return "", history, error_msg, current_model

        send_btn.click(
            on_send_message,
            inputs=[msg, chatbot, current_model_display],
            outputs=[msg, chatbot, stats_display, current_model_display]
        )

        msg.submit(
            on_send_message,
            inputs=[msg, chatbot, current_model_display],
            outputs=[msg, chatbot, stats_display, current_model_display]
        )

        # 4. è®°å¿†ç®¡ç†
        def refresh_memory():
            return ai_chat.get_memory_summary()

        refresh_memory_btn.click(
            refresh_memory,
            outputs=[memory_display]
        )

        clear_memory_btn.click(
            ai_chat.clear_memory,
            outputs=[memory_display]
        )

        # 5. æ¸…ç©ºå½“å‰å¯¹è¯
        clear_chat_btn.click(
            lambda: ([], "å¯¹è¯å·²æ¸…ç©º", ai_chat.get_current_model() or "æœªåŠ è½½"),
            outputs=[chatbot, stats_display, current_model_display]
        )

        # 6. QAæŒ‰é’®äº‹ä»¶
        def start_qa_interface():
            result = ai_chat.launch_qa_interface()
            url = "æœªè·å–åˆ°URL"
            if "http://" in result:
                import re
                urls = re.findall(r'http://[^\s]+', result)
                if urls:
                    url = urls[0]
            return result, url

        def stop_qa_interface():
            result = ai_chat.stop_qa_interface()
            return result, "å·²åœæ­¢"

        def open_qa_browser():
            url = f"http://127.0.0.1:{ai_chat.qa_port}"
            webbrowser.open(url)
            return f"âœ… å·²å°è¯•æ‰“å¼€QAç•Œé¢: {url}"

        qa_start_btn.click(
            start_qa_interface,
            outputs=[qa_status, qa_url_display]
        )

        qa_stop_btn.click(
            stop_qa_interface,
            outputs=[qa_status, qa_url_display]
        )

        qa_open_btn.click(
            open_qa_browser,
            outputs=[qa_status]
        )

        # 7. é¡µé¢åŠ è½½æ—¶åˆå§‹åŒ–
        demo.load(
            lambda: ai_chat.get_memory_summary(),
            outputs=[memory_display]
        )

    return demo

def main():
    print("ğŸŒ å¯åŠ¨å¢å¼ºç‰ˆAIåŠ©æ‰‹...")
    print("ğŸ’¡ æ–°åŠŸèƒ½:")
    print("  â€¢ æ”¯æŒåˆ‡æ¢0.5B/1.5B/3Bæ¨¡å‹")
    print("  â€¢ å¯¹è¯è®°å¿†ä¿å­˜ä¸æŸ¥çœ‹")
    print("  â€¢ å®æ—¶æ˜¾å­˜æ£€æµ‹")
    print("  â€¢ ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯")
    print("-" * 50)
    print(f"ğŸ’¡ æœ¬åœ°è®¿é—®: http://127.0.0.1:{UI_CONFIG['server_port']}")
    print(f"ğŸŒ å±€åŸŸç½‘è®¿é—®: http://ä½ çš„IP:{UI_CONFIG['server_port']}")

    demo = create_enhanced_interface()

    demo.launch(
        server_name=UI_CONFIG["server_name"],
        server_port=UI_CONFIG["server_port"],
        share=UI_CONFIG["share"],
        show_error=True,
        debug=False
    )


if __name__ == "__main__":
    main()