"""
æ–‡æ¡£é—®ç­”æå–å™¨ - ä¼˜åŒ–ç‰ˆæœ¬
æ”¯æŒMacBERT/BERT-wwm/ERNIEï¼Œæ”¹è¿›é—®é¢˜è¯†åˆ«å‡†ç¡®æ€§
"""
import os
import json
import torch
import sys
import re
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any
import time
import numpy as np

# # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¿…é¡»åœ¨import transformersä¹‹å‰ï¼‰
# os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
# os.environ["TOKENIZERS_PARALLELISM"] = "false"  # é¿å…tokenizerè­¦å‘Š
# import os

# å¼ºåˆ¶ä½¿ç”¨å›½å†…é•œåƒç«™ï¼ˆæ·»åŠ è¿™ä¸‰è¡Œï¼‰
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HOME"] = "D:/huggingface_cache"  # å¯è‡ªå®šä¹‰ç¼“å­˜è·¯å¾„
os.environ["TRANSFORMERS_OFFLINE"] = "0"
os.environ["HF_DATASETS_OFFLINE"] = "0"


print("=" * 60)
print("ğŸ“„ æ–‡æ¡£é—®ç­”æå–å™¨ - ä¼˜åŒ–ç‰ˆæœ¬åˆå§‹åŒ–")
print("=" * 60)

try:
    from transformers import (
        AutoTokenizer,
        AutoModelForQuestionAnswering,
        pipeline,
        QuestionAnsweringPipeline
    )
    TRANSFORMERS_AVAILABLE = True
    print("âœ… Transformersåº“å¯ç”¨")
except ImportError as e:
    TRANSFORMERS_AVAILABLE = False
    print(f"âŒ Transformersåº“å¯¼å…¥å¤±è´¥: {e}")


class TextPreprocessor:
    """æ–‡æœ¬é¢„å¤„ç†å™¨"""

    @staticmethod
    def clean_text(text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬"""
        # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œç¬¦
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n+', '\n', text)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™æ ‡ç‚¹
        text = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€ï¼ˆï¼‰ã€Šã€‹ã€Œã€ã€ã€‘]', ' ', text)
        return text.strip()

    @staticmethod
    def split_long_text(text: str, max_length: int = 500) -> List[str]:
        """å°†é•¿æ–‡æœ¬åˆ†å‰²æˆå°å—"""
        if len(text) <= max_length:
            return [text]

        # å°è¯•æŒ‰æ ‡ç‚¹åˆ†å‰²
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿï¼›])', text)
        chunks = []
        current_chunk = ""

        for i in range(0, len(sentences), 2):
            sentence = sentences[i] + (sentences[i+1] if i+1 < len(sentences) else "")
            if len(current_chunk) + len(sentence) <= max_length:
                current_chunk += sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence

                # å¦‚æœå•ä¸ªå¥å­å°±è¶…è¿‡max_lengthï¼Œå¼ºåˆ¶åˆ†å‰²
                if len(current_chunk) > max_length:
                    # æŒ‰å­—ç¬¦åˆ†å‰²
                    for j in range(0, len(current_chunk), max_length):
                        chunks.append(current_chunk[j:j+max_length])
                    current_chunk = ""

        if current_chunk:
            chunks.append(current_chunk)

        return chunks

    @staticmethod
    def normalize_question(question: str) -> str:
        """æ ‡å‡†åŒ–é—®é¢˜"""
        question = question.strip()
        # ç§»é™¤é—®é¢˜æœ«å°¾æ ‡ç‚¹
        question = re.sub(r'[ã€‚ï¼ï¼Ÿï¼›ï¼šï¼Œã€]$', '', question)
        # ç¡®ä¿é—®é¢˜ä»¥é—®å·ç»“å°¾ï¼ˆå¦‚æœä¸æ˜¯é™ˆè¿°å¥ï¼‰
        if not question.endswith('ï¼Ÿ') and not question.endswith('?') and not question.endswith('ã€‚'):
            question += 'ï¼Ÿ'
        return question


class DocumentQA:
    """ä¼˜åŒ–ç‰ˆå•æ¨¡å‹æ–‡æ¡£é—®ç­”æå–å™¨"""

    def __init__(self, model_name_or_path: str, device: str = None):
        """
        åˆå§‹åŒ–æ–‡æ¡£é—®ç­”æ¨¡å‹

        Args:
            model_name_or_path: æ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„
            device: è¿è¡Œè®¾å¤‡ ('cuda', 'cpu', æˆ– Noneè‡ªåŠ¨é€‰æ‹©)
        """
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError("è¯·å…ˆå®‰è£…transformersåº“: pip install transformers")

        self.model_name = model_name_or_path
        self.device = device if device else self._auto_select_device()
        self.preprocessor = TextPreprocessor()

        print(f"ğŸ”§ åˆå§‹åŒ–QAæ¨¡å‹: {model_name_or_path}")
        print(f"ğŸ® ä½¿ç”¨è®¾å¤‡: {self.device}")

        # è®°å½•åŠ è½½æ—¶é—´
        start_time = time.time()

        try:
            # åŠ è½½tokenizerå’Œæ¨¡å‹
            print("ğŸ“¥ åŠ è½½tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name_or_path,
                trust_remote_code=True
            )

            print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
            # å°è¯•ä½¿ç”¨å®‰å…¨çš„safetensorsæ ¼å¼åŠ è½½
            try:
                self.model = AutoModelForQuestionAnswering.from_pretrained(
                    model_name_or_path,
                    trust_remote_code=True,
                    use_safetensors=True  # ä¼˜å…ˆä½¿ç”¨safetensors
                )
            except:
                # å¦‚æœsafetensorså¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šæ–¹å¼
                print("âš ï¸  safetensorsåŠ è½½å¤±è´¥ï¼Œå°è¯•æ™®é€šåŠ è½½...")
                self.model = AutoModelForQuestionAnswering.from_pretrained(
                    model_name_or_path,
                    trust_remote_code=True
                )

            # åˆ›å»ºpipelineï¼Œè®¾ç½®æ›´è¯¦ç»†çš„å‚æ•°
            print("ğŸ”— åˆ›å»ºé—®ç­”pipeline...")
            self.qa_pipeline = pipeline(
                "question-answering",
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if self.device == "cuda" else -1,
                batch_size=1
            )

            # è·å–æ¨¡å‹çš„æœ€å¤§é•¿åº¦
            self.max_length = self.tokenizer.model_max_length
            if self.max_length > 512:  # é™åˆ¶æœ€å¤§é•¿åº¦
                self.max_length = 512

            load_time = time.time() - start_time
            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ! è€—æ—¶: {load_time:.2f}ç§’")
            print(f"ğŸ“ æœ€å¤§åºåˆ—é•¿åº¦: {self.max_length}")

        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å°è¯•å¤‡ç”¨åŠ è½½æ–¹å¼
            try:
                print("ğŸ”„ å°è¯•å¤‡ç”¨åŠ è½½æ–¹å¼...")
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_name_or_path,
                    use_fast=True
                )
                self.model = AutoModelForQuestionAnswering.from_pretrained(
                    model_name_or_path
                ).to(self.device)

                self.qa_pipeline = pipeline(
                    "question-answering",
                    model=self.model,
                    tokenizer=self.tokenizer,
                    device=0 if self.device == "cuda" else -1
                )
                print("âœ… å¤‡ç”¨åŠ è½½æ–¹å¼æˆåŠŸ!")
            except Exception as e2:
                raise RuntimeError(f"æ‰€æœ‰åŠ è½½æ–¹å¼éƒ½å¤±è´¥: {e2}")

    def _auto_select_device(self) -> str:
        """è‡ªåŠ¨é€‰æ‹©è®¾å¤‡"""
        # å¼ºåˆ¶ä¼˜å…ˆä½¿ç”¨GPU
        try:
            # ä¼˜å…ˆå°è¯•DirectMLï¼ˆAMDæ˜¾å¡ï¼‰
            import torch_directml
            dml_device = torch_directml.device()
            print(f"âœ… æ£€æµ‹åˆ°AMDæ˜¾å¡ï¼Œä½¿ç”¨DirectMLåŠ é€Ÿ")
            return dml_device
        except ImportError:
            print("âš ï¸  æœªå®‰è£…torch_directmlï¼Œå°è¯•ä½¿ç”¨CUDA")

        # å…¶æ¬¡å°è¯•CUDAï¼ˆNVIDIAæ˜¾å¡ï¼‰
        if torch.cuda.is_available():
            print("âœ… æ£€æµ‹åˆ°NVIDIAæ˜¾å¡ï¼Œä½¿ç”¨CUDAåŠ é€Ÿ")
            return "cuda"

        # æœ€åä½¿ç”¨CPU
        print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPU")
        return "cpu"
    # def _auto_select_device(self) -> str:
    #     """è‡ªåŠ¨é€‰æ‹©è®¾å¤‡"""
    #     if torch.cuda.is_available():
    #         return "cuda"
    #     elif hasattr(torch, 'directml'):  # æ£€æŸ¥DirectMLæ”¯æŒ
    #         try:
    #             import torch_directml
    #             return torch_directml.device()
    #         except:
    #             return "cpu"
    #     else:
    #         return "cpu"

    def extract_answer(
        self,
        context: str,
        question: str,
        max_answer_length: int = 150,  # å¢åŠ é»˜è®¤é•¿åº¦
        top_k: int = 5,  # å¢åŠ é»˜è®¤top_k
        confidence_threshold: float = 0.1,  # ç½®ä¿¡åº¦é˜ˆå€¼
        handle_long_document: bool = True
    ) -> List[Dict]:
        """
        ä»æ–‡æ¡£ä¸­æå–ç­”æ¡ˆï¼ˆä¼˜åŒ–ç‰ˆï¼‰

        Args:
            context: æ–‡æ¡£å†…å®¹
            question: é—®é¢˜
            max_answer_length: æœ€å¤§ç­”æ¡ˆé•¿åº¦
            top_k: è¿”å›å‰kä¸ªæœ€å¯èƒ½çš„ç­”æ¡ˆ
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            handle_long_document: æ˜¯å¦å¤„ç†é•¿æ–‡æ¡£

        Returns:
            ç­”æ¡ˆåˆ—è¡¨ï¼Œæ¯ä¸ªç­”æ¡ˆåŒ…å«: text, score, start, end
        """
        try:
            # é¢„å¤„ç†æ–‡æœ¬
            context = self.preprocessor.clean_text(context)
            question = self.preprocessor.normalize_question(question)

            print(f"ğŸ“Š é¢„å¤„ç†å: æ–‡æ¡£é•¿åº¦={len(context)}, é—®é¢˜='{question}'")

            # å¤„ç†é•¿æ–‡æ¡£
            answers = []
            if handle_long_document and len(context) > 800:
                print("ğŸ“ æ–‡æ¡£è¾ƒé•¿ï¼Œå¯ç”¨åˆ†å—å¤„ç†...")
                chunks = self.preprocessor.split_long_text(context, max_length=800)

                for chunk_idx, chunk in enumerate(chunks):
                    if len(chunk) < 20:  # è·³è¿‡å¤ªçŸ­çš„å—
                        continue

                    print(f"  å¤„ç†åˆ†å— {chunk_idx+1}/{len(chunks)} (é•¿åº¦: {len(chunk)})")

                    chunk_answers = self._extract_from_chunk(
                        chunk, question, max_answer_length, top_k
                    )

                    for ans in chunk_answers:
                        # è°ƒæ•´ç­”æ¡ˆä½ç½®å¹¶æ·»åŠ åˆ°ç»“æœ
                        if ans.get("score", 0) > confidence_threshold:
                            answers.append(ans)

                # åˆå¹¶ç›¸ä¼¼çš„ç­”æ¡ˆ
                answers = self._merge_similar_answers(answers)
            else:
                # ç›´æ¥æå–
                answers = self._extract_from_chunk(
                    context, question, max_answer_length, top_k
                )

            # è¿‡æ»¤å’Œæ’åº
            filtered_answers = []
            for result in answers:
                if result.get("score", 0) > confidence_threshold:
                    filtered_answers.append(result)

            # æŒ‰ç½®ä¿¡åº¦æ’åº
            filtered_answers.sort(key=lambda x: x.get("score", 0), reverse=True)

            # é™åˆ¶æ•°é‡
            filtered_answers = filtered_answers[:top_k]

            print(f"âœ… æ‰¾åˆ° {len(filtered_answers)} ä¸ªæœ‰æ•ˆç­”æ¡ˆ")
            return filtered_answers

        except Exception as e:
            print(f"âŒ ç­”æ¡ˆæå–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []

    def _extract_from_chunk(
        self,
        context: str,
        question: str,
        max_answer_length: int,
        top_k: int
    ) -> List[Dict]:
        """ä»å•ä¸ªæ–‡æ¡£å—æå–ç­”æ¡ˆ"""
        try:
            # ä½¿ç”¨pipelineæå–ç­”æ¡ˆ
            results = self.qa_pipeline(
                {
                    "context": context,
                    "question": question
                },
                top_k=top_k,
                max_answer_len=max_answer_length,
                handle_impossible_answer=False,  # ä¸å¤„ç†æ— æ³•å›ç­”çš„æƒ…å†µ
                max_seq_len=self.max_length,
                doc_stride=128  # å¢åŠ æ­¥é•¿
            )

            # ç¡®ä¿ç»“æœæ€»æ˜¯åˆ—è¡¨
            if not isinstance(results, list):
                results = [results]

            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            for result in results:
                if result.get("answer"):
                    # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆè½¬æ¢ä¸ºç™¾åˆ†æ¯”ï¼‰
                    score = result.get("score", 0)

                    # åå¤„ç†ç­”æ¡ˆæ–‡æœ¬
                    answer_text = self._postprocess_answer(result["answer"])

                    if answer_text:  # ç¡®ä¿ç­”æ¡ˆéç©º
                        formatted_results.append({
                            "text": answer_text,
                            "score": round(score, 4),
                            "confidence": round(score * 100, 2),
                            "start": result.get("start", 0),
                            "end": result.get("end", 0),
                            "context": context  # ä¿ç•™ä¸Šä¸‹æ–‡ç”¨äºè°ƒè¯•
                        })

            return formatted_results

        except Exception as e:
            print(f"âŒ åˆ†å—æå–å¤±è´¥: {e}")
            return []

    def _postprocess_answer(self, answer: str) -> str:
        """åå¤„ç†ç­”æ¡ˆæ–‡æœ¬"""
        if not answer:
            return ""

        # æ¸…ç†ç­”æ¡ˆ
        answer = answer.strip()
        answer = re.sub(r'^\s*[.,ï¼Œã€‚!ï¼?ï¼Ÿ;ï¼›:ï¼š]\s*', '', answer)
        answer = re.sub(r'\s+', ' ', answer)

        # ç§»é™¤ä¸å®Œæ•´çš„å¥å­
        if len(answer) < 2:  # å¤ªçŸ­
            return ""

        return answer

    def _merge_similar_answers(self, answers: List[Dict], similarity_threshold: float = 0.8) -> List[Dict]:
        """åˆå¹¶ç›¸ä¼¼çš„ç­”æ¡ˆ"""
        if not answers:
            return []

        merged = []
        used = [False] * len(answers)

        for i in range(len(answers)):
            if used[i]:
                continue

            current = answers[i]
            similar_group = [current]
            used[i] = True

            # å¯»æ‰¾ç›¸ä¼¼ç­”æ¡ˆ
            for j in range(i + 1, len(answers)):
                if not used[j]:
                    # ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆåŸºäºé‡å ï¼‰
                    text1 = current["text"]
                    text2 = answers[j]["text"]

                    # è®¡ç®—Jaccardç›¸ä¼¼åº¦
                    set1 = set(text1)
                    set2 = set(text2)
                    intersection = len(set1 & set2)
                    union = len(set1 | set2)

                    if union > 0 and intersection / union > similarity_threshold:
                        similar_group.append(answers[j])
                        used[j] = True

            # åˆå¹¶ç›¸ä¼¼ç­”æ¡ˆï¼ˆå–ç½®ä¿¡åº¦æœ€é«˜çš„ï¼‰
            if similar_group:
                best_answer = max(similar_group, key=lambda x: x.get("score", 0))
                merged.append(best_answer)

        return merged

    def batch_extract(self, contexts: List[str], questions: List[str],
                      **kwargs) -> List[List[Dict]]:
        """æ‰¹é‡æå–ç­”æ¡ˆ"""
        results = []
        for context, question in zip(contexts, questions):
            answers = self.extract_answer(context, question, **kwargs)
            results.append(answers)
        return results

    def get_model_info(self) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "model_name": self.model_name,
            "device": str(self.device),
            "max_length": self.max_length,
            "tokenizer_vocab_size": self.tokenizer.vocab_size,
            "model_parameters": sum(p.numel() for p in self.model.parameters())
        }


class QAExtractorManager:
    """QAæ¨¡å‹ç®¡ç†å™¨ï¼ˆå¤šæ¨¡å‹æ”¯æŒï¼‰"""

    # é¢„å®šä¹‰çš„æ¨¡å‹é…ç½® - ä½¿ç”¨CMRC2018å¾®è°ƒæ¨¡å‹
    QA_MODEL_CONFIGS = {
         # ==== é—®ç­”ä¸“ç”¨æ¨¡å‹ï¼ˆå¼ºçƒˆæ¨èï¼‰====
        "uer-roberta-qa": {
            "name": "uer/roberta-base-chinese-extractive-qa",
            "local_path": r"F:\py_work\AMD_AI_Project\AMD_AI_Project\models\qa_models\roberta-base-chinese-extractive-qa",
            "description": "UER RoBERTaæŠ½å–å¼é—®ç­”æ¨¡å‹ï¼ˆé—®ç­”ä¸“ç”¨ï¼Œå¼ºçƒˆæ¨èï¼‰",
            "max_context_length": 512,
            "recommended_top_k": 3,
            "confidence_threshold": 0.1
        },
        "macbert-cmrc": {
            "name": "hfl/chinese-macbert-base-cmrc2018",
            "local_path": r"F:\py_work\AMD_AI_Project\AMD_AI_Project\models\qa_models\macbert-cmrc",
            "description": "MacBERTåœ¨CMRC2018ä¸Šå¾®è°ƒçš„é—®ç­”æ¨¡å‹ï¼ˆæ¨èï¼‰",
            "max_context_length": 512,
            "recommended_top_k": 3,
            "confidence_threshold": 0.1
        },
        "bert-wwm-cmrc": {
            "name": "hfl/chinese-bert-wwm-ext-cmrc2018",
            "local_path": r"F:\py_work\AMD_AI_Project\AMD_AI_Project\models\qa_models\bert-wwm-cmrc",
            "description": "BERT-wwmåœ¨CMRC2018ä¸Šå¾®è°ƒçš„é—®ç­”æ¨¡å‹",
            "max_context_length": 512,
            "recommended_top_k": 3,
            "confidence_threshold": 0.1
        },
        "roberta-cmrc": {
            "name": "hfl/chinese-roberta-wwm-ext-cmrc2018",
            "local_path": r"F:\py_work\AMD_AI_Project\AMD_AI_Project\models\qa_models\roberta-cmrc",
            "description": "RoBERTaåœ¨CMRC2018ä¸Šå¾®è°ƒçš„é—®ç­”æ¨¡å‹",
            "max_context_length": 512,
            "recommended_top_k": 3,
            "confidence_threshold": 0.1
        },
        "macbert-base": {
            "name": "hfl/chinese-macbert-base",
            "local_path": r"F:\py_work\AMD_AI_Project\AMD_AI_Project\models\qa_models\macbert",
            "description": "åŸºç¡€MacBERTæ¨¡å‹ï¼ˆé€šç”¨ï¼‰",
            "max_context_length": 512,
            "recommended_top_k": 5,
            "confidence_threshold": 0.05  # é™ä½é˜ˆå€¼
        },
        "bert-wwm-base": {
            "name": "hfl/chinese-bert-wwm-ext",
            "local_path": r"F:\py_work\AMD_AI_Project\AMD_AI_Project\models\qa_models\bert-wwm",
            "description": "åŸºç¡€BERT-wwmæ¨¡å‹",
            "max_context_length": 512,
            "recommended_top_k": 5,
            "confidence_threshold": 0.05
        },
        "ernie-base": {
            "name": "nghuyong/ernie-3.0-base-zh",
            "local_path": r"F:\py_work\AMD_AI_Project\AMD_AI_Project\models\qa_models\ernie",
            "description": "ERNIE 3.0åŸºç¡€æ¨¡å‹",
            "max_context_length": 512,
            "recommended_top_k": 5,
            "confidence_threshold": 0.05
        }
    }

    def __init__(self):
        """åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨"""
        self.models = {}  # å·²åŠ è½½çš„æ¨¡å‹å­—å…¸
        self.current_model = None
        self.current_model_key = None
        self.default_params = {
            "top_k": 3,
            "max_answer_length": 150,
            "confidence_threshold": 0.1,
            "handle_long_document": True
        }

        # è‡ªåŠ¨å‘ç°å¾®è°ƒæ¨¡å‹
        self.finetuned_models = self._discover_finetuned_models()

        print("âœ… QAæ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

        # åˆå¹¶é¢„å®šä¹‰æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹
        all_models = list(self.QA_MODEL_CONFIGS.keys()) + list(self.finetuned_models.keys())
        print(f"ğŸ“Š æ”¯æŒ {len(all_models)} ä¸ªæ¨¡å‹:")
        for model in all_models:
            print(f"   â€¢ {model}")

        # å°†uer-roberta-qaè®¾ä¸ºæ¨èæ¨¡å‹
        self.recommended_model = "uer-roberta-qa"
        print(f"ğŸŒŸ æ¨èæ¨¡å‹: {self.recommended_model}")

    def _discover_finetuned_models(self) -> Dict:
        """è‡ªåŠ¨å‘ç°å¾®è°ƒæ¨¡å‹"""
        finetuned_models = {}
        try:
            # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆåŠ¨æ€è®¡ç®—ï¼‰
            import sys
            from pathlib import Path

            # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
            current_file = Path(__file__)
            # è®¡ç®—é¡¹ç›®æ ¹ç›®å½•ï¼šå½“å‰æ–‡ä»¶ -> src/qa/ -> src/ -> é¡¹ç›®æ ¹ç›®å½•
            project_root_path = current_file.parent.parent.parent

            # å¾®è°ƒæ¨¡å‹ç›®å½•
            finetuned_dir = project_root_path / "finetuned_models"

            # å¦‚æœåŠ¨æ€è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ç»å¯¹è·¯å¾„
            if not finetuned_dir.exists():
                finetuned_dir = Path(r"F:\py_work\AMD_AI_Project\AMD_AI_Project\finetuned_models")

            if not finetuned_dir.exists():
                print(f"âš ï¸  å¾®è°ƒç›®å½•ä¸å­˜åœ¨: {finetuned_dir}")
                return finetuned_models

            print(f"ğŸ” æ‰«æå¾®è°ƒæ¨¡å‹ç›®å½•: {finetuned_dir}")

            for model_dir in finetuned_dir.iterdir():
                if model_dir.is_dir():
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„æ¨¡å‹æ–‡ä»¶
                    required_files = ["config.json", "model.safetensors", "tokenizer.json"]
                    # å…è®¸ .safetensors æˆ– .bin æ ¼å¼
                    has_config = (model_dir / "config.json").exists()
                    has_model = ((model_dir / "model.safetensors").exists() or
                                 (model_dir / "pytorch_model.bin").exists())
                    has_tokenizer = (model_dir / "tokenizer.json").exists()

                    if has_config and has_model and has_tokenizer:
                        model_key = f"finetuned-{model_dir.name}"
                        finetuned_models[model_key] = {
                            "name": str(model_dir),
                            "local_path": str(model_dir),
                            "description": f"å¾®è°ƒæ¨¡å‹: {model_dir.name}",
                            "max_context_length": 512,
                            "recommended_top_k": 3,
                            "confidence_threshold": 0.05
                        }
                        print(f"   âœ… å‘ç°å¾®è°ƒæ¨¡å‹: {model_key}")

            return finetuned_models

        except Exception as e:
            print(f"âŒ å‘ç°å¾®è°ƒæ¨¡å‹æ—¶å‡ºé”™: {e}")
            return finetuned_models

        # # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆåŠ¨æ€è®¡ç®—ï¼‰
        # current_file = Path(__file__)
        # project_root_path = current_file.parent.parent.parent  # src/qa/ -> src/ -> project_root
        # finetuned_dir = project_root_path / "finetuned_models"
        #
        # # æˆ–è€…ä½¿ç”¨ç»å¯¹è·¯å¾„
        # # finetuned_dir = Path(r"F:\py_work\AMD_AI_Project\AMD_AI_Project\finetuned_models")
        #
        # if not finetuned_dir.exists():
        #     print(f"âš ï¸  å¾®è°ƒç›®å½•ä¸å­˜åœ¨: {finetuned_dir}")
        #     return finetuned_models
        #
        # print(f"ğŸ” æ‰«æå¾®è°ƒæ¨¡å‹ç›®å½•: {finetuned_dir}")
        #
        # for model_dir in finetuned_dir.iterdir():
        #     if model_dir.is_dir():
        #         # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„æ¨¡å‹æ–‡ä»¶
        #         required_files = ["config.json", "model.safetensors", "tokenizer.json"]
        #         has_required = all((model_dir / f).exists() for f in required_files)
        #
        #         if has_required:
        #             model_key = f"finetuned-{model_dir.name}"
        #             finetuned_models[model_key] = {
        #                 "name": str(model_dir),
        #                 "local_path": str(model_dir),
        #                 "description": f"å¾®è°ƒæ¨¡å‹: {model_dir.name}",
        #                 "max_context_length": 512,
        #                 "recommended_top_k": 3,
        #                 "confidence_threshold": 0.05
        #             }
        #             print(f"   âœ… å‘ç°å¾®è°ƒæ¨¡å‹: {model_key}")
        #
        # return finetuned_models

    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆåŒ…æ‹¬å¾®è°ƒæ¨¡å‹ï¼‰"""
        all_models = list(self.QA_MODEL_CONFIGS.keys()) + list(self.finetuned_models.keys())

        # å°†å¾®è°ƒæ¨¡å‹æ’åœ¨å‰é¢
        finetuned_keys = list(self.finetuned_models.keys())
        other_keys = [m for m in all_models if m not in finetuned_keys]

        return finetuned_keys + other_keys

    def get_all_model_configs(self) -> Dict:
        """è·å–æ‰€æœ‰æ¨¡å‹é…ç½®ï¼ˆåŒ…æ‹¬å¾®è°ƒæ¨¡å‹ï¼‰"""
        all_configs = self.QA_MODEL_CONFIGS.copy()
        all_configs.update(self.finetuned_models)
        return all_configs

    def get_model_info(self, model_key: str) -> str:
        """è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯"""
        # å…ˆæ£€æŸ¥é¢„å®šä¹‰æ¨¡å‹
        if model_key in self.QA_MODEL_CONFIGS:
            config = self.QA_MODEL_CONFIGS[model_key]
        elif model_key in self.finetuned_models:
            config = self.finetuned_models[model_key]
        else:
            return f"âŒ æœªçŸ¥æ¨¡å‹: {model_key}"

        info = f"""
    ğŸ“Š æ¨¡å‹ä¿¡æ¯: {model_key}
    â€¢ æè¿°: {config['description']}
    â€¢ æœ€å¤§ä¸Šä¸‹æ–‡: {config['max_context_length']} tokens
    â€¢ æ¨è top_k: {config.get('recommended_top_k', 3)}
    â€¢ ç½®ä¿¡åº¦é˜ˆå€¼: {config.get('confidence_threshold', 0.1)}
    â€¢ æœ¬åœ°è·¯å¾„: {config['local_path']}
            """

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        if model_key in self.models:
            model_info = self.models[model_key].get_model_info()
            info += f"\nâ€¢ çŠ¶æ€: âœ… å·²åŠ è½½"
            info += f"\nâ€¢ è®¾å¤‡: {model_info['device']}"
            info += f"\nâ€¢ æœ€å¤§é•¿åº¦: {model_info['max_length']}"
            info += f"\nâ€¢ å‚æ•°é‡: {model_info['model_parameters']:,}"
        else:
            info += f"\nâ€¢ çŠ¶æ€: â³ æœªåŠ è½½"

            # æ£€æŸ¥æœ¬åœ°æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            local_path = Path(config["local_path"])
            if local_path.exists():
                info += f"\nâ€¢ æœ¬åœ°æ–‡ä»¶: âœ… å­˜åœ¨"
            else:
                info += f"\nâ€¢ æœ¬åœ°æ–‡ä»¶: âŒ ä¸å­˜åœ¨"

        return info.strip()

    def load_model(self, model_key: str, force_reload: bool = False) -> str:
        """
        åŠ è½½æŒ‡å®šæ¨¡å‹

        Args:
            model_key: æ¨¡å‹é”®å
            force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½

        Returns:
            åŠ è½½çŠ¶æ€æ¶ˆæ¯
        """
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯å¾®è°ƒæ¨¡å‹
        if model_key in self.finetuned_models:
            config = self.finetuned_models[model_key]
            print(f"ğŸ” åŠ è½½å¾®è°ƒæ¨¡å‹: {model_key}")
        elif model_key in self.QA_MODEL_CONFIGS:
            config = self.QA_MODEL_CONFIGS[model_key]
            print(f"ğŸ” åŠ è½½é¢„å®šä¹‰æ¨¡å‹: {model_key}")
        else:
            return f"âŒ æ— æ•ˆçš„æ¨¡å‹é€‰æ‹©: {model_key}"

        # æ£€æŸ¥æ˜¯å¦å·²åŠ è½½
        if model_key in self.models and not force_reload:
            if self.current_model_key == model_key:
                return f"âœ… {model_key} å·²åŠ è½½ï¼Œæ— éœ€é‡æ–°åŠ è½½"

        print("=" * 50)
        print(f"ğŸ”„ åŠ è½½QAæ¨¡å‹: {model_key}")
        print(f"ğŸ“‚ æ¨¡å‹è·¯å¾„: {config['local_path']}")
        print("=" * 50)

        try:
            # æ£€æŸ¥æœ¬åœ°è·¯å¾„æ˜¯å¦å­˜åœ¨
            local_path = Path(config["local_path"])
            model_path = None

            if local_path.exists() and any(local_path.iterdir()):
                # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„æ¨¡å‹æ–‡ä»¶
                required_files = ["config.json", "tokenizer.json"]
                # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ï¼ˆæ”¯æŒä¸¤ç§æ ¼å¼ï¼‰
                model_files = ["model.safetensors", "pytorch_model.bin"]

                has_required = all((local_path / f).exists() for f in required_files)
                has_model = any((local_path / f).exists() for f in model_files)

                if has_required and has_model:
                    model_path = str(local_path)
                    print(f"ğŸ“ ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {model_path}")
                else:
                    print(f"âš ï¸ æœ¬åœ°æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´")
                    missing_files = []
                    for f in required_files:
                        if not (local_path / f).exists():
                            missing_files.append(f)
                    for f in model_files:
                        if not (local_path / f).exists():
                            missing_files.append(f)
                    print(f"   ç¼ºå°‘æ–‡ä»¶: {missing_files}")

                    # å¦‚æœæ˜¯é¢„å®šä¹‰æ¨¡å‹ï¼Œå°è¯•ä»ç½‘ç»œä¸‹è½½
                    if model_key in self.QA_MODEL_CONFIGS:
                        model_path = config["name"]
                        print(f"ğŸŒ å°è¯•ä¸‹è½½æ¨¡å‹: {model_path}")
                    else:
                        return f"âŒ å¾®è°ƒæ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´ï¼Œè¯·é‡æ–°å¾®è°ƒ"
            else:
                # ä½¿ç”¨HuggingFaceæ¨¡å‹ï¼ˆä»…é¢„å®šä¹‰æ¨¡å‹ï¼‰
                if model_key in self.QA_MODEL_CONFIGS:
                    model_path = config["name"]
                    print(f"ğŸŒ ä¸‹è½½æ¨¡å‹: {model_path}")
                else:
                    return f"âŒ å¾®è°ƒæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {local_path}"

            # åŠ è½½æ¨¡å‹
            start_time = time.time()

            try:
                model = DocumentQA(model_path)
            except Exception as e:
                print(f"âŒ DocumentQAåŠ è½½å¤±è´¥: {e}")
                # å°è¯•ç›´æ¥ä½¿ç”¨transformersåŠ è½½
                print("ğŸ”„ å°è¯•ç›´æ¥åŠ è½½...")
                from transformers import AutoTokenizer, AutoModelForQuestionAnswering
                tokenizer = AutoTokenizer.from_pretrained(model_path)
                qa_model = AutoModelForQuestionAnswering.from_pretrained(model_path)

                # åˆ›å»ºè‡ªå®šä¹‰çš„DocumentQAå¯¹è±¡
                model = DocumentQA.__new__(DocumentQA)
                model.model_name = model_path
                model.device = "cuda" if torch.cuda.is_available() else "cpu"
                model.preprocessor = TextPreprocessor()
                model.tokenizer = tokenizer
                model.model = qa_model.to(model.device)
                model.max_length = tokenizer.model_max_length

                # åˆ›å»ºpipeline
                from transformers import pipeline
                model.qa_pipeline = pipeline(
                    "question-answering",
                    model=qa_model,
                    tokenizer=tokenizer,
                    device=0 if model.device == "cuda" else -1,
                    batch_size=1
                )

            load_time = time.time() - start_time

            # æ›´æ–°çŠ¶æ€
            self.models[model_key] = model
            self.current_model = model
            self.current_model_key = model_key

            # æ›´æ–°é»˜è®¤å‚æ•°
            if "confidence_threshold" in config:
                self.default_params["confidence_threshold"] = config.get("confidence_threshold", 0.1)
            if "recommended_top_k" in config:
                self.default_params["top_k"] = config.get("recommended_top_k", 3)

            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ! è€—æ—¶: {load_time:.2f}ç§’")
            print(f"âš™ï¸ æ¨èå‚æ•°: top_k={self.default_params['top_k']}, "
                  f"ç½®ä¿¡åº¦é˜ˆå€¼={self.default_params['confidence_threshold']}")

            return (f"âœ… {model_key} åŠ è½½æˆåŠŸ!\n"
                    f"â€¢ è®¾å¤‡: {model.device}\n"
                    f"â€¢ è€—æ—¶: {load_time:.2f}ç§’\n"
                    f"â€¢ æ¨è top_k: {self.default_params['top_k']}")

        except Exception as e:
            error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
            print(error_msg)

            # æä¾›å…·ä½“å»ºè®®
            if "ConnectionError" in str(e):
                error_msg += "\nğŸ’¡ ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ä½¿ç”¨æœ¬åœ°æ¨¡å‹"
            elif "401" in str(e):
                error_msg += "\nğŸ’¡ è®¤è¯å¤±è´¥ï¼Œå¯èƒ½éœ€è¦è®¿é—®ä»¤ç‰Œ"
            elif "404" in str(e):
                error_msg += "\nğŸ’¡ æ¨¡å‹ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥æ¨¡å‹åç§°"
            elif "safetensors" in str(e):
                error_msg += "\nğŸ’¡ å¯èƒ½æ˜¯æ¨¡å‹æ ¼å¼é—®é¢˜ï¼Œå°è¯•é‡æ–°å¾®è°ƒ"

            return error_msg
    def extract_answer(
        self,
        context: str,
        question: str,
        **kwargs
    ) -> Dict:
        """
        æå–ç­”æ¡ˆï¼ˆå¢å¼ºç‰ˆï¼‰

        Returns:
            Dict: åŒ…å«çŠ¶æ€ã€ç­”æ¡ˆå’Œç»Ÿè®¡ä¿¡æ¯
        """
        if self.current_model is None:
            return {
                "status": "error",
                "message": "è¯·å…ˆåŠ è½½æ¨¡å‹",
                "answers": [],
                "stats": {},
                "suggestion": "è¯·ä»å·¦ä¾§é€‰æ‹©å¹¶åŠ è½½ä¸€ä¸ªæ¨¡å‹"
            }

        try:
            start_time = time.time()

            # åˆå¹¶å‚æ•°
            params = self.default_params.copy()
            params.update(kwargs)

            # éªŒè¯è¾“å…¥
            if not context or not context.strip():
                return {
                    "status": "error",
                    "message": "æ–‡æ¡£å†…å®¹ä¸èƒ½ä¸ºç©º",
                    "answers": [],
                    "stats": {}
                }

            if not question or not question.strip():
                return {
                    "status": "error",
                    "message": "é—®é¢˜ä¸èƒ½ä¸ºç©º",
                    "answers": [],
                    "stats": {}
                }

            print(f"ğŸ” å¼€å§‹æå–ç­”æ¡ˆ...")
            print(f"   æ–‡æ¡£é•¿åº¦: {len(context)} å­—ç¬¦")
            print(f"   é—®é¢˜: {question}")
            print(f"   å‚æ•°: top_k={params.get('top_k')}, "
                  f"ç½®ä¿¡åº¦é˜ˆå€¼={params.get('confidence_threshold')}")

            # æå–ç­”æ¡ˆ
            answers = self.current_model.extract_answer(
                context=context,
                question=question,
                max_answer_length=params.get("max_answer_length", 150),
                top_k=params.get("top_k", 3),
                confidence_threshold=params.get("confidence_threshold", 0.1),
                handle_long_document=params.get("handle_long_document", True)
            )

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            process_time = time.time() - start_time
            context_length = len(context)
            question_length = len(question)

            stats = {
                "process_time": round(process_time, 2),
                "context_length": context_length,
                "question_length": question_length,
                "answers_found": len(answers),
                "model_used": self.current_model_key,
                "avg_confidence": 0
            }

            # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
            if answers:
                total_confidence = sum(ans.get("confidence", 0) for ans in answers)
                stats["avg_confidence"] = round(total_confidence / len(answers), 2)

            # ç”Ÿæˆå»ºè®®
            suggestion = ""
            if not answers:
                suggestion = (
                    "ğŸ’¡ å»ºè®®:\n"
                    "1. ç¡®ä¿æ–‡æ¡£åŒ…å«ç›¸å…³ä¿¡æ¯\n"
                    "2. å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„é—®é¢˜\n"
                    "3. è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼æˆ–top_kå‚æ•°\n"
                    "4. å°è¯•å…¶ä»–æ¨¡å‹ï¼ˆå¦‚macbert-cmrcï¼‰"
                )
            elif stats["avg_confidence"] < 20:
                suggestion = (
                    "ğŸ’¡ ç½®ä¿¡åº¦è¾ƒä½ï¼Œå»ºè®®:\n"
                    "1. æ£€æŸ¥æ–‡æ¡£ä¸é—®é¢˜çš„ç›¸å…³æ€§\n"
                    "2. é™ä½ç½®ä¿¡åº¦é˜ˆå€¼\n"
                    "3. ä½¿ç”¨ä¸“é—¨å¾®è°ƒçš„é—®ç­”æ¨¡å‹"
                )

            if answers:
                return {
                    "status": "success",
                    "message": f"æ‰¾åˆ° {len(answers)} ä¸ªç›¸å…³ç­”æ¡ˆ",
                    "answers": answers,
                    "stats": stats,
                    "suggestion": suggestion
                }
            else:
                return {
                    "status": "info",
                    "message": "æœªæ‰¾åˆ°ç›¸å…³ç­”æ¡ˆ",
                    "answers": [],
                    "stats": stats,
                    "suggestion": suggestion
                }

        except Exception as e:
            print(f"âŒ æå–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

            return {
                "status": "error",
                "message": f"æå–å¤±è´¥: {str(e)}",
                "answers": [],
                "stats": {},
                "suggestion": "è¯·æ£€æŸ¥è¾“å…¥æ ¼å¼æˆ–å°è¯•é‡æ–°åŠ è½½æ¨¡å‹"
            }

    def batch_extract(self, contexts: List[str], questions: List[str], **kwargs) -> List[Dict]:
        """æ‰¹é‡æå–ç­”æ¡ˆ"""
        if self.current_model is None:
            return [{"error": "æ¨¡å‹æœªåŠ è½½"}]

        results = []
        for i, (context, question) in enumerate(zip(contexts, questions)):
            print(f"ğŸ” æ‰¹é‡å¤„ç† {i+1}/{len(contexts)}...")
            result = self.extract_answer(context, question, **kwargs)
            results.append(result)

        return results

    def get_current_model(self) -> str:
        """è·å–å½“å‰æ¨¡å‹"""
        if self.current_model_key:
            return f"{self.current_model_key} (å·²åŠ è½½)"
        return "æœªåŠ è½½æ¨¡å‹"

    def get_default_params(self) -> Dict:
        """è·å–é»˜è®¤å‚æ•°"""
        return self.default_params.copy()

    def clear_model(self, model_key: str = None):
        """æ¸…ç†æ¨¡å‹"""
        if model_key and model_key in self.models:
            del self.models[model_key]
            if self.current_model_key == model_key:
                self.current_model = None
                self.current_model_key = None
            print(f"âœ… å·²æ¸…ç†æ¨¡å‹: {model_key}")
        elif not model_key and self.current_model_key:
            key = self.current_model_key
            if key in self.models:
                del self.models[key]
            self.current_model = None
            self.current_model_key = None
            print(f"âœ… å·²æ¸…ç†å½“å‰æ¨¡å‹")

    def get_recommended_model(self) -> str:
        """è·å–æ¨èæ¨¡å‹"""
        recommended_models = ["uer-roberta-qa"]
        for model in recommended_models:
            if model in self.QA_MODEL_CONFIGS:
                return model
        return list(self.QA_MODEL_CONFIGS.keys())[0] if self.QA_MODEL_CONFIGS else "macbert-base"


# åˆ›å»ºå…¨å±€å®ä¾‹
qa_manager = QAExtractorManager()