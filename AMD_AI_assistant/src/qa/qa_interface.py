"""
æ–‡æ¡£é—®ç­”ç•Œé¢ - ä¼˜åŒ–ç‰ˆGradio Webç•Œé¢
"""
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent
sys.path.append(str(project_root))

print("=" * 60)
print("ğŸ“„ æ–‡æ¡£é—®ç­”ç•Œé¢ - ä¼˜åŒ–ç‰ˆåˆå§‹åŒ–")
print("=" * 60)

import gradio as gr
import json
import time
from datetime import datetime
from src.qa.qa_extractor import QAExtractorManager


class QAChat:
    """æ–‡æ¡£é—®ç­”èŠå¤©ç±»ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""

    def __init__(self):
        self.qa_manager = QAExtractorManager()
        self.conversation_history = []  # ä¿å­˜é—®ç­”å†å²
        self.max_history_size = 20  # æœ€å¤§å†å²è®°å½•æ•°

        # åŠ è½½æ¨èæ¨¡å‹
        self.recommended_model = self.qa_manager.get_recommended_model()
        print(f"ğŸŒŸ æ¨èæ¨¡å‹: {self.recommended_model}")

    def get_model_list(self):
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        models = self.qa_manager.get_available_models()
        # å°†æ¨èæ¨¡å‹æ”¾åœ¨ç¬¬ä¸€ä½
        if self.recommended_model in models:
            models.remove(self.recommended_model)
            models.insert(0, self.recommended_model)
        return models

    def get_model_info(self, model_key):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return self.qa_manager.get_model_info(model_key)

    def load_model(self, model_key, show_info=True):
        """åŠ è½½æ¨¡å‹"""
        result = self.qa_manager.load_model(model_key)

        if show_info and "âœ…" in result:
            # è·å–æ¨¡å‹è¯¦æƒ…
            model_info = self.qa_manager.get_model_info(model_key)
            result += f"\n\n{model_info}"

        return result

    def extract_answer(self, document, question, top_k=3, max_length=150,
                       confidence_threshold=0.1, handle_long_document=True):
        """æå–ç­”æ¡ˆï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if not document.strip():
            return {
                "status": "error",
                "message": "è¯·è¾“å…¥æ–‡æ¡£å†…å®¹",
                "answers": [],
                "stats": {},
                "suggestion": "è¯·åœ¨å·¦ä¾§è¾“å…¥æ–‡æ¡£å†…å®¹"
            }

        if not question.strip():
            return {
                "status": "error",
                "message": "è¯·è¾“å…¥é—®é¢˜",
                "answers": [],
                "stats": {},
                "suggestion": "è¯·åœ¨ä¸‹æ–¹è¾“å…¥æ‚¨çš„é—®é¢˜"
            }

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # æå–ç­”æ¡ˆ
        result = self.qa_manager.extract_answer(
            context=document,
            question=question,
            top_k=top_k,
            max_answer_length=max_length,
            confidence_threshold=confidence_threshold,
            handle_long_document=handle_long_document
        )

        # æ·»åŠ å¤„ç†æ—¶é—´
        if "stats" in result:
            result["stats"]["total_time"] = round(time.time() - start_time, 2)

        # ä¿å­˜åˆ°å†å²ï¼ˆé™åˆ¶å†å²å¤§å°ï¼‰
        if result["status"] in ["success", "info"]:
            history_entry = {
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "document_preview": document[:200] + ("..." if len(document) > 200 else ""),
                "question": question,
                "answers": result["answers"],
                "stats": result["stats"],
                "status": result["status"]
            }

            self.conversation_history.append(history_entry)

            # é™åˆ¶å†å²å¤§å°
            if len(self.conversation_history) > self.max_history_size:
                self.conversation_history = self.conversation_history[-self.max_history_size:]

        return result

    def get_history_summary(self, detailed=False):
        """è·å–å†å²æ‘˜è¦"""
        if not self.conversation_history:
            return "ğŸ“š æš‚æ— é—®ç­”å†å²\n\nğŸ’¡ å¼€å§‹æ‚¨çš„ç¬¬ä¸€æ¬¡é—®ç­”å§ï¼"

        summary = f"ğŸ“š æœ€è¿‘é—®ç­”å†å² ({len(self.conversation_history)} æ¡):\n\n"

        for i, conv in enumerate(reversed(self.conversation_history[-10:]), 1):
            summary += f"ğŸ”¹ {conv['timestamp']}\n"
            summary += f"   é—®é¢˜: {conv['question']}\n"

            if conv['answers']:
                best_answer = conv['answers'][0]
                summary += f"   æœ€ä½³ç­”æ¡ˆ: {best_answer['text'][:80]}...\n"
                summary += f"   ç½®ä¿¡åº¦: {best_answer.get('confidence', 0):.1f}%\n"
            else:
                summary += f"   ç»“æœ: {conv['status']}\n"

            if conv.get('stats'):
                summary += f"   è€—æ—¶: {conv['stats'].get('process_time', 0)}ç§’\n"

            summary += "-" * 50 + "\n"

        return summary.strip()

    def get_detailed_history(self):
        """è·å–è¯¦ç»†å†å²"""
        if not self.conversation_history:
            return "æš‚æ— è¯¦ç»†å†å²è®°å½•"

        detailed = "ğŸ“‹ è¯¦ç»†é—®ç­”å†å²:\n\n"
        for i, conv in enumerate(reversed(self.conversation_history), 1):
            detailed += f"è®°å½• #{i}\n"
            detailed += f"æ—¶é—´: {conv['timestamp']}\n"
            detailed += f"çŠ¶æ€: {conv['status']}\n"
            detailed += f"é—®é¢˜: {conv['question']}\n"

            if conv['answers']:
                detailed += "ç­”æ¡ˆ:\n"
                for j, ans in enumerate(conv['answers'], 1):
                    detailed += f"  {j}. {ans['text']}\n"
                    detailed += f"     ç½®ä¿¡åº¦: {ans.get('confidence', 0):.1f}%\n"
            else:
                detailed += "ç­”æ¡ˆ: æœªæ‰¾åˆ°\n"

            detailed += "\n" + "="*60 + "\n\n"

        return detailed.strip()

    def clear_history(self):
        """æ¸…ç©ºå†å²"""
        self.conversation_history.clear()
        return "ğŸ—‘ï¸ é—®ç­”å†å²å·²æ¸…ç©º"

    def get_current_model(self):
        """è·å–å½“å‰æ¨¡å‹"""
        return self.qa_manager.get_current_model()

    def get_default_params(self):
        """è·å–é»˜è®¤å‚æ•°"""
        return self.qa_manager.get_default_params()

    def export_history(self, format_type="json"):
        """å¯¼å‡ºå†å²è®°å½•"""
        if not self.conversation_history:
            return "æ— å†å²è®°å½•å¯å¯¼å‡º"

        if format_type == "json":
            return json.dumps(self.conversation_history, ensure_ascii=False, indent=2)
        elif format_type == "txt":
            txt_output = "æ–‡æ¡£é—®ç­”å†å²è®°å½•\n"
            txt_output += "=" * 40 + "\n\n"
            for conv in self.conversation_history:
                txt_output += f"æ—¶é—´: {conv['timestamp']}\n"
                txt_output += f"é—®é¢˜: {conv['question']}\n"
                txt_output += f"æ–‡æ¡£é¢„è§ˆ: {conv['document_preview']}\n"
                txt_output += f"çŠ¶æ€: {conv['status']}\n"
                if conv['answers']:
                    txt_output += "ç­”æ¡ˆ:\n"
                    for ans in conv['answers']:
                        txt_output += f"  - {ans['text']} (ç½®ä¿¡åº¦: {ans.get('confidence', 0):.1f}%)\n"
                txt_output += "\n" + "-"*40 + "\n\n"
            return txt_output
        else:
            return "ä¸æ”¯æŒè¯¥æ ¼å¼"


def format_answers(result):
    """æ ¼å¼åŒ–ç­”æ¡ˆè¾“å‡ºï¼ˆå¢å¼ºç‰ˆï¼‰"""
    if "error" in result:
        return f"âŒ {result['error']}"

    status = result.get("status", "")
    message = result.get("message", "")
    answers = result.get("answers", [])
    stats = result.get("stats", {})
    suggestion = result.get("suggestion", "")

    if status == "error":
        return f"âŒ {message}\n\nğŸ’¡ å»ºè®®: {suggestion if suggestion else 'è¯·æ£€æŸ¥è¾“å…¥æˆ–é‡æ–°åŠ è½½æ¨¡å‹'}"
    elif status == "info":
        return f"â„¹ï¸ {message}\n\nğŸ’¡ å»ºè®®: {suggestion if suggestion else 'è¯·è°ƒæ•´é—®é¢˜æˆ–å‚æ•°åé‡è¯•'}"

    # æ ¼å¼åŒ–ç­”æ¡ˆ
    output = f"âœ… {message}\n\n"

    if not answers:
        output += "âš ï¸ æœªæå–åˆ°æœ‰æ•ˆç­”æ¡ˆ\n"
    else:
        for i, answer in enumerate(answers, 1):
            confidence = answer.get("confidence", answer.get("score", 0) * 100)
            output += f"ğŸ“„ ç­”æ¡ˆ {i} (ç½®ä¿¡åº¦: {confidence:.1f}%):\n"
            output += f"   {answer['text']}\n"
            if i < len(answers):
                output += "-" * 50 + "\n"

    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    if stats:
        output += "\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:\n"
        output += f"â€¢ å¤„ç†æ—¶é—´: {stats.get('process_time', 0)}ç§’\n"
        if 'total_time' in stats:
            output += f"â€¢ æ€»è€—æ—¶: {stats.get('total_time', 0)}ç§’\n"
        output += f"â€¢ æ–‡æ¡£é•¿åº¦: {stats.get('context_length', 0)}å­—ç¬¦\n"
        output += f"â€¢ é—®é¢˜é•¿åº¦: {stats.get('question_length', 0)}å­—ç¬¦\n"
        output += f"â€¢ æ‰¾åˆ°ç­”æ¡ˆ: {stats.get('answers_found', 0)}ä¸ª\n"
        output += f"â€¢ å¹³å‡ç½®ä¿¡åº¦: {stats.get('avg_confidence', 0)}%\n"
        output += f"â€¢ ä½¿ç”¨æ¨¡å‹: {stats.get('model_used', 'N/A')}\n"

    # æ·»åŠ å»ºè®®
    if suggestion:
        output += f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:\n{suggestion}\n"

    return output


def format_stats(result):
    """æ ¼å¼åŒ–ç»Ÿè®¡ä¿¡æ¯"""
    if "error" in result or "stats" not in result or not result["stats"]:
        return "ğŸ“Š ç­‰å¾…æå–..."

    stats = result["stats"]
    stats_text = f"""
ğŸ“Š å¤„ç†ç»Ÿè®¡:
â€¢ è€—æ—¶: {stats.get('process_time', 0)}ç§’
â€¢ æ–‡æ¡£: {stats.get('context_length', 0)}å­—ç¬¦
â€¢ é—®é¢˜: {stats.get('question_length', 0)}å­—ç¬¦
â€¢ ç­”æ¡ˆ: {stats.get('answers_found', 0)}ä¸ª
â€¢ ç½®ä¿¡åº¦: {stats.get('avg_confidence', 0)}%
â€¢ æ¨¡å‹: {stats.get('model_used', 'N/A')}
"""
    return stats_text.strip()


def create_qa_interface():
    """åˆ›å»ºQAç•Œé¢ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    qa_chat = QAChat()
    default_params = qa_chat.get_default_params()

    with gr.Blocks(title="ğŸ“„ æ–‡æ¡£é—®ç­”åŠ©æ‰‹ - å®Œæ•´ç‰ˆ") as demo:
        gr.Markdown("""
        # ğŸ“„ æ–‡æ¡£é—®ç­”åŠ©æ‰‹ - å®Œæ•´ç‰ˆ
        ### æ”¯æŒæ–‡æ¡£é—®ç­” + æ¨¡å‹å¾®è°ƒåŠŸèƒ½
        """)

        with gr.Tabs():
            # ===== ç¬¬ä¸€ä¸ªæ ‡ç­¾é¡µï¼šæ–‡æ¡£é—®ç­” =====
            with gr.Tab("ğŸ’¬ æ–‡æ¡£é—®ç­”"):
                with gr.Row():
                    # å·¦ä¾§æ§åˆ¶é¢æ¿
                    with gr.Column(scale=1):
                        gr.Markdown("### âš™ï¸ æ¨¡å‹æ§åˆ¶")

                        # æ¨¡å‹é€‰æ‹©
                        model_selector = gr.Dropdown(
                            choices=qa_chat.get_model_list(),
                            value=qa_chat.get_model_list()[0] if qa_chat.get_model_list() else None,
                            label="é€‰æ‹©QAæ¨¡å‹",
                            interactive=True,
                            info="ğŸŒŸ æ¨èä½¿ç”¨å·²å¾®è°ƒçš„æ¨¡å‹"
                        )

                        model_info = gr.Textbox(
                            label="ğŸ“Š æ¨¡å‹ä¿¡æ¯",
                            value=qa_chat.get_model_info(
                                qa_chat.get_model_list()[0]) if qa_chat.get_model_list() else "æ— å¯ç”¨æ¨¡å‹",
                            lines=8,
                            interactive=False
                        )

                        with gr.Row():
                            load_btn = gr.Button("ğŸš€ åŠ è½½æ¨¡å‹", variant="primary", scale=2)
                            refresh_models_btn = gr.Button("ğŸ”„ åˆ·æ–°", scale=1)

                        # å‚æ•°è®¾ç½®
                        gr.Markdown("### âš™ï¸ å‚æ•°è®¾ç½®")

                        with gr.Accordion("é«˜çº§å‚æ•°è®¾ç½®", open=False):
                            top_k_slider = gr.Slider(
                                minimum=1,
                                maximum=10,
                                value=default_params.get("top_k", 3),
                                step=1,
                                label="è¿”å›ç­”æ¡ˆæ•°é‡ (top_k)"
                            )

                            max_length_slider = gr.Slider(
                                minimum=50,
                                maximum=300,
                                value=default_params.get("max_answer_length", 150),
                                step=10,
                                label="æœ€å¤§ç­”æ¡ˆé•¿åº¦"
                            )

                            confidence_slider = gr.Slider(
                                minimum=0.01,
                                maximum=0.5,
                                value=default_params.get("confidence_threshold", 0.1),
                                step=0.01,
                                label="ç½®ä¿¡åº¦é˜ˆå€¼",
                                info="å€¼è¶Šä½ï¼Œè¿”å›çš„ç­”æ¡ˆè¶Šå¤š"
                            )

                            handle_long_doc = gr.Checkbox(
                                label="å¤„ç†é•¿æ–‡æ¡£",
                                value=default_params.get("handle_long_document", True),
                                info="è‡ªåŠ¨åˆ†å‰²é•¿æ–‡æ¡£"
                            )

                        status_display = gr.Textbox(
                            label="ğŸ“ˆ æ¨¡å‹çŠ¶æ€",
                            value="è¯·é€‰æ‹©æ¨¡å‹å¹¶ç‚¹å‡»åŠ è½½",
                            lines=3,
                            interactive=False
                        )

                        gr.Markdown("---")

                        # å†å²ç®¡ç†
                        gr.Markdown("### ğŸ“š é—®ç­”å†å²")

                        history_display = gr.Textbox(
                            label="å†å²è®°å½•",
                            value=qa_chat.get_history_summary(),
                            lines=8,
                            interactive=False
                        )

                        with gr.Row():
                            refresh_history_btn = gr.Button("ğŸ”„ åˆ·æ–°å†å²")
                            clear_history_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå†å²", variant="stop")
                            export_btn = gr.Button("ğŸ“¥ å¯¼å‡ºå†å²")

                        current_model_display = gr.Textbox(
                            label="ğŸ¤– å½“å‰æ¨¡å‹",
                            value="æœªåŠ è½½",
                            interactive=False
                        )

                    # å³ä¾§é—®ç­”åŒºåŸŸ
                    with gr.Column(scale=2):
                        gr.Markdown("### ğŸ“„ æ–‡æ¡£é—®ç­”åŒºåŸŸ")

                        # ç¤ºä¾‹æŒ‰é’®
                        with gr.Row():
                            example_btn1 = gr.Button("ğŸ“‹ ç¤ºä¾‹1: å…¬å¸ä»‹ç»", size="sm")
                            example_btn2 = gr.Button("ğŸ“‹ ç¤ºä¾‹2: æŠ€æœ¯æ–‡æ¡£", size="sm")
                            example_btn3 = gr.Button("ğŸ“‹ ç¤ºä¾‹3: æ–°é—»å†…å®¹", size="sm")

                        # æ–‡æ¡£è¾“å…¥
                        document_input = gr.Textbox(
                            label="ğŸ“„ æ–‡æ¡£å†…å®¹",
                            placeholder="è¯·åœ¨æ­¤å¤„ç²˜è´´æˆ–è¾“å…¥æ–‡æ¡£å†…å®¹...\nï¼ˆæ”¯æŒé•¿æ–‡æ¡£ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨åˆ†å‰²å¤„ç†ï¼‰",
                            lines=15,
                            max_lines=30,
                            info="å»ºè®®æ–‡æ¡£é•¿åº¦åœ¨100-5000å­—ç¬¦ä¹‹é—´"
                        )

                        # é—®é¢˜è¾“å…¥
                        question_input = gr.Textbox(
                            label="â“ é—®é¢˜",
                            placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...\nä¾‹å¦‚ï¼šè¿™ç¯‡æ–‡ç« çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                            lines=3
                        )

                        with gr.Row():
                            extract_btn = gr.Button("ğŸ” æå–ç­”æ¡ˆ", variant="primary", scale=2)
                            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºè¾“å…¥", scale=1)

                        # ç­”æ¡ˆè¾“å‡º
                        answer_output = gr.Textbox(
                            label="ğŸ“ æå–ç»“æœ",
                            placeholder="ç­”æ¡ˆå°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...",
                            lines=20,
                            interactive=False
                        )

                        # ç»Ÿè®¡ä¿¡æ¯
                        stats_display = gr.Textbox(
                            label="ğŸ“Š å¤„ç†ç»Ÿè®¡",
                            value="ç­‰å¾…æå–...",
                            lines=4,
                            interactive=False
                        )

            # ===== ç¬¬äºŒä¸ªæ ‡ç­¾é¡µï¼šæ¨¡å‹å¾®è°ƒ =====
            with gr.Tab("ğŸ”§ æ¨¡å‹å¾®è°ƒ"):
                gr.Markdown("""
                ### ğŸ¯ æ¨¡å‹å¾®è°ƒè®¾ç½®
                **åŠŸèƒ½è¯´æ˜**ï¼šåœ¨ç°æœ‰æ¨¡å‹åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶æ›´é€‚åˆæ‚¨çš„æ–‡æ¡£ç±»å‹
                **æ³¨æ„**ï¼šå¾®è°ƒéœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œå»ºè®®å‡†å¤‡å¥½æ•°æ®é›†åå†å¼€å§‹
                """)

                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("#### åŸºç¡€è®¾ç½®")

                        base_model_select = gr.Dropdown(
                            choices=["macbert-base", "bert-wwm-base"],
                            value="macbert-base",
                            label="é€‰æ‹©åŸºç¡€æ¨¡å‹",
                            info="é€‰æ‹©è¦å¾®è°ƒçš„åŸºç¡€æ¨¡å‹"
                        )

                        dataset_path = gr.Textbox(
                            label="æ•°æ®é›†è·¯å¾„",
                            value=r"F:\py_work\AMD_AI_Project\AMD_AI_Project\finetune_data\custom_sample",
                            placeholder="è¾“å…¥æ•°æ®é›†è·¯å¾„",
                            info="åŒ…å«train.json, dev.json, test.jsonçš„ç›®å½•"
                        )

                        output_dir = gr.Textbox(
                            label="è¾“å‡ºç›®å½•",
                            value=r"F:\py_work\AMD_AI_Project\AMD_AI_Project\finetuned_models",
                            placeholder="å¾®è°ƒæ¨¡å‹è¾“å‡ºç›®å½•",
                            info="å¾®è°ƒåçš„æ¨¡å‹å°†ä¿å­˜åˆ°æ­¤ç›®å½•"
                        )

                        model_name_suffix = gr.Textbox(
                            label="æ¨¡å‹åç§°åç¼€",
                            value="my_finetuned",
                            placeholder="å¦‚: my_finetuned",
                            info="å°†æ·»åŠ åˆ°æ¨¡å‹åç§°åé¢ï¼Œç”¨äºåŒºåˆ†"
                        )

                    with gr.Column(scale=1):
                        gr.Markdown("#### è®­ç»ƒå‚æ•°")

                        epochs = gr.Slider(
                            minimum=1, maximum=10, value=3,
                            step=1, label="è®­ç»ƒè½®æ•° (epochs)",
                            info="å»ºè®®3-5è½®"
                        )

                        batch_size = gr.Slider(
                            minimum=1, maximum=16, value=8,
                            step=1, label="æ‰¹æ¬¡å¤§å° (batch_size)",
                            info="æ ¹æ®æ˜¾å­˜è°ƒæ•´"
                        )

                        learning_rate = gr.Number(
                            value=3e-5, label="å­¦ä¹ ç‡ (learning_rate)",
                            info="å»ºè®®3e-5"
                        )

                        device_select = gr.Dropdown(
                            choices=["auto", "cpu", "directml", "cuda"],
                            value="auto",
                            label="è®­ç»ƒè®¾å¤‡",
                            info="auto: è‡ªåŠ¨æ£€æµ‹, directml: AMDæ˜¾å¡"
                        )

                gr.Markdown("---")

                with gr.Row():
                    start_finetune_btn = gr.Button("ğŸš€ ç”Ÿæˆå¾®è°ƒå‘½ä»¤", variant="primary", size="lg")
                    show_finetune_cmd_btn = gr.Button("ğŸ“‹ æ˜¾ç¤ºå¾®è°ƒå‘½ä»¤", size="lg")

                finetune_output = gr.Textbox(
                    label="å¾®è°ƒè¾“å‡º/å‘½ä»¤",
                    lines=15,
                    interactive=False,
                    placeholder="å¾®è°ƒå‘½ä»¤å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ..."
                )

                # å¾®è°ƒè¿›åº¦ï¼ˆæ¨¡æ‹Ÿï¼‰
                progress_bar = gr.Slider(
                    minimum=0, maximum=100, value=0,
                    label="è®­ç»ƒè¿›åº¦", interactive=False,
                    visible=False  # æš‚æ—¶éšè—
                )

                gr.Markdown("---")

                with gr.Row():
                    refresh_finetuned_btn = gr.Button("ğŸ”„ åˆ·æ–°å¾®è°ƒæ¨¡å‹åˆ—è¡¨")
                    open_finetune_dir_btn = gr.Button("ğŸ“ æ‰“å¼€å¾®è°ƒç›®å½•")

                finetuned_models_list = gr.Textbox(
                    label="å·²å‘ç°çš„å¾®è°ƒæ¨¡å‹",
                    lines=8,
                    interactive=False,
                    value="ç‚¹å‡»'åˆ·æ–°å¾®è°ƒæ¨¡å‹åˆ—è¡¨'æŸ¥çœ‹"
                )

                gr.Markdown("""
                ### ğŸ’¡ å¾®è°ƒä½¿ç”¨è¯´æ˜
                1. **å‡†å¤‡æ•°æ®é›†**ï¼šå°†æ‚¨çš„æ–‡æ¡£å’Œé—®ç­”å¯¹æ•´ç†æˆCMRC2018æ ¼å¼
                2. **è®¾ç½®å‚æ•°**ï¼šè°ƒæ•´è®­ç»ƒå‚æ•°ï¼Œé€‰æ‹©åˆé€‚çš„è®¾å¤‡
                3. **ç”Ÿæˆå‘½ä»¤**ï¼šç‚¹å‡»"ç”Ÿæˆå¾®è°ƒå‘½ä»¤"è·å–è®­ç»ƒå‘½ä»¤
                4. **è¿è¡Œå¾®è°ƒ**ï¼šåœ¨ç»ˆç«¯/å‘½ä»¤è¡Œä¸­è¿è¡Œç”Ÿæˆçš„å‘½ä»¤
                5. **åŠ è½½æ¨¡å‹**ï¼šå¾®è°ƒå®Œæˆåï¼Œåˆ·æ–°æ¨¡å‹åˆ—è¡¨å¹¶åŠ è½½æ–°æ¨¡å‹

                **æ•°æ®é›†æ ¼å¼**ï¼šéœ€è¦ä¸‰ä¸ªæ–‡ä»¶ï¼š`train.json`ã€`dev.json`ã€`test.json`
                """)

        # ===== äº‹ä»¶ç»‘å®š =====

        # é¡µé¢åŠ è½½æ—¶åˆå§‹åŒ–
        def on_page_load():
            return qa_chat.get_history_summary()

        demo.load(
            on_page_load,
            outputs=[history_display]
        )

        # 1. æ¨¡å‹é€‰æ‹©å™¨æ›´æ–°ä¿¡æ¯
        def update_model_info(model_key):
            return qa_chat.get_model_info(model_key)

        model_selector.change(
            update_model_info,
            inputs=[model_selector],
            outputs=[model_info]
        )

        # 2. åŠ è½½æ¨¡å‹
        def on_load_model(model_key):
            status = qa_chat.load_model(model_key)
            current_model = qa_chat.get_current_model()
            return status, current_model, qa_chat.get_history_summary()

        load_btn.click(
            on_load_model,
            inputs=[model_selector],
            outputs=[status_display, current_model_display, history_display]
        )

        # 3. åˆ·æ–°æ¨¡å‹åˆ—è¡¨
        def refresh_model_list():
            return gr.update(choices=qa_chat.get_model_list()), \
                qa_chat.get_model_info(qa_chat.get_model_list()[0]) if qa_chat.get_model_list() else "æ— å¯ç”¨æ¨¡å‹"

        refresh_models_btn.click(
            refresh_model_list,
            outputs=[model_selector, model_info]
        )

        # 4. æå–ç­”æ¡ˆ
        def on_extract_answer(document, question, top_k, max_length, confidence, handle_long):
            if not document.strip() or not question.strip():
                return "è¯·è¾“å…¥æ–‡æ¡£å†…å®¹å’Œé—®é¢˜", "ç­‰å¾…æå–...", qa_chat.get_current_model()

            # æå–ç­”æ¡ˆ
            result = qa_chat.extract_answer(
                document, question, top_k, max_length, confidence, handle_long
            )

            # æ ¼å¼åŒ–è¾“å‡º
            formatted_answer = format_answers(result)
            stats_text = format_stats(result)

            return formatted_answer, stats_text, qa_chat.get_current_model()

        extract_btn.click(
            on_extract_answer,
            inputs=[document_input, question_input, top_k_slider, max_length_slider,
                    confidence_slider, handle_long_doc],
            outputs=[answer_output, stats_display, current_model_display]
        )

        # 5. æ¸…ç©ºè¾“å…¥
        clear_btn.click(
            lambda: ("", "", "", "ç­‰å¾…æå–...", qa_chat.get_current_model()),
            outputs=[document_input, question_input, answer_output,
                     stats_display, current_model_display]
        )

        # 6. ç¤ºä¾‹æŒ‰é’®
        def load_example(example_id):
            examples = {
                1: {
                    "document": """é˜¿é‡Œå·´å·´é›†å›¢æˆç«‹äº1999å¹´ï¼Œæ˜¯ä¸€å®¶ä»¥ç”µå­å•†åŠ¡ä¸ºæ ¸å¿ƒä¸šåŠ¡çš„äº’è”ç½‘å…¬å¸ã€‚å…¬å¸æ€»éƒ¨ä½äºä¸­å›½æ­å·ï¼Œä¸šåŠ¡æ¶µç›–ç”µå•†ã€äº‘è®¡ç®—ã€æ•°å­—åª’ä½“å’Œå¨±ä¹ç­‰å¤šä¸ªé¢†åŸŸã€‚

é˜¿é‡Œå·´å·´çš„ä½¿å‘½æ˜¯è®©å¤©ä¸‹æ²¡æœ‰éš¾åšçš„ç”Ÿæ„ã€‚é€šè¿‡æ·˜å®ã€å¤©çŒ«ç­‰å¹³å°ï¼Œå…¬å¸ä¸ºæ•°ç™¾ä¸‡å•†å®¶å’Œæ•°äº¿æ¶ˆè´¹è€…æä¾›äº¤æ˜“æœåŠ¡ã€‚æ­¤å¤–ï¼Œé˜¿é‡Œäº‘å·²æˆä¸ºå…¨çƒé¢†å…ˆçš„äº‘è®¡ç®—æœåŠ¡æä¾›å•†ä¹‹ä¸€ã€‚

2023è´¢å¹´ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ€»è¥æ”¶è¾¾åˆ°8686äº¿å…ƒï¼Œå‡€åˆ©æ¶¦ä¸º725äº¿å…ƒã€‚å…¬å¸æŒç»­æŠ•èµ„äºæŠ€æœ¯åˆ›æ–°ï¼Œç‰¹åˆ«æ˜¯åœ¨äººå·¥æ™ºèƒ½å’Œå¤§æ•°æ®é¢†åŸŸã€‚""",
                    "question": "é˜¿é‡Œå·´å·´çš„æ€»éƒ¨åœ¨å“ªé‡Œï¼Ÿ"
                },
                2: {
                    "document": """æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚ç¥ç»ç½‘ç»œç”±è¾“å…¥å±‚ã€éšè—å±‚å’Œè¾“å‡ºå±‚ç»„æˆï¼Œæ¯å±‚åŒ…å«å¤šä¸ªç¥ç»å…ƒã€‚

åå‘ä¼ æ’­ç®—æ³•æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œçš„å…³é”®æŠ€æœ¯ï¼Œé€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°çš„æ¢¯åº¦æ¥æ›´æ–°ç½‘ç»œæƒé‡ã€‚å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°åŒ…æ‹¬Sigmoidã€ReLUå’ŒTanhã€‚

å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸“é—¨ç”¨äºå¤„ç†å›¾åƒæ•°æ®ï¼Œè€Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰åˆ™æ“…é•¿å¤„ç†åºåˆ—æ•°æ®ã€‚Transformeræ¶æ„è¿‘å¹´æ¥åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚""",
                    "question": "ä»€ä¹ˆæ˜¯å·ç§¯ç¥ç»ç½‘ç»œçš„ä¸»è¦åº”ç”¨ï¼Ÿ"
                },
                3: {
                    "document": """ä¸­å›½å›½å®¶èˆªå¤©å±€è¿‘æ—¥å®£å¸ƒï¼Œå«¦å¨¥å…­å·æ¢æµ‹å™¨æˆåŠŸåœ¨æœˆçƒèƒŒé¢ç€é™†ã€‚è¿™æ˜¯äººç±»å†å²ä¸Šé¦–æ¬¡åœ¨æœˆçƒèƒŒé¢è¿›è¡Œçš„é‡‡æ ·è¿”å›ä»»åŠ¡ã€‚

å«¦å¨¥å…­å·ä»»åŠ¡çš„ä¸»è¦ç§‘å­¦ç›®æ ‡åŒ…æ‹¬ï¼šé‡‡é›†æœˆçƒèƒŒé¢çš„åœŸå£¤å’Œå²©çŸ³æ ·æœ¬ï¼Œè¿›è¡Œç°åœºåˆ†æï¼Œå¹¶å°†æ ·æœ¬è¿”å›åœ°çƒã€‚æ¢æµ‹å™¨æºå¸¦äº†å¤šç§ç§‘å­¦ä»ªå™¨ï¼ŒåŒ…æ‹¬å…¨æ™¯ç›¸æœºã€å…‰è°±ä»ªå’Œæ¢åœ°é›·è¾¾ã€‚

æ­¤æ¬¡ä»»åŠ¡çš„æˆåŠŸå®æ–½ï¼Œæ ‡å¿—ç€ä¸­å›½åœ¨æ·±ç©ºæ¢æµ‹é¢†åŸŸå–å¾—äº†é‡è¦è¿›å±•ï¼Œä¸ºæœªæ¥çš„æœˆçƒç§‘ç ”ç«™å»ºè®¾å’Œè½½äººç™»æœˆä»»åŠ¡å¥ å®šäº†åšå®åŸºç¡€ã€‚""",
                    "question": "å«¦å¨¥å…­å·çš„ä¸»è¦ä»»åŠ¡æ˜¯ä»€ä¹ˆï¼Ÿ"
                }
            }

            if example_id in examples:
                example = examples[example_id]
                return example["document"], example["question"]
            return "", ""

        example_btn1.click(lambda: load_example(1), outputs=[document_input, question_input])
        example_btn2.click(lambda: load_example(2), outputs=[document_input, question_input])
        example_btn3.click(lambda: load_example(3), outputs=[document_input, question_input])

        # 7. å†å²ç®¡ç†
        def refresh_history():
            return qa_chat.get_history_summary()

        refresh_history_btn.click(
            refresh_history,
            outputs=[history_display]
        )

        clear_history_btn.click(
            lambda: (qa_chat.clear_history(), qa_chat.get_history_summary()),
            outputs=[status_display, history_display]
        )

        def on_export_history():
            return qa_chat.export_history("txt")

        export_btn.click(
            on_export_history,
            outputs=[answer_output]
        )

        # ===== å¾®è°ƒç›¸å…³äº‹ä»¶ =====

        # 8. ç”Ÿæˆå¾®è°ƒå‘½ä»¤
        def generate_finetune_command(base_model, dataset, output, suffix, epochs_val, batch_size_val, lr, device):
            """ç”Ÿæˆå¾®è°ƒå‘½ä»¤"""
            import datetime

            # æ„å»ºè¾“å‡ºè·¯å¾„
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            model_output = f"{output}/{base_model.replace('-base', '')}_{suffix}_{timestamp}"

            # æ„å»ºå‘½ä»¤
            cmd = f"""# å¾®è°ƒå‘½ä»¤ - å¤åˆ¶åˆ°ç»ˆç«¯è¿è¡Œ
cd /d "F:\\py_work\\AMD_AI_Project\\AMD_AI_Project"

python finetune_scripts/finetune_qa.py \\
  --base_model models/qa_models/{base_model.split('-')[0]} \\
  --dataset "{dataset}" \\
  --output_dir "{model_output}" \\
  --epochs {epochs_val} \\
  --batch_size {batch_size_val} \\
  --learning_rate {lr} \\
  --device {device}

# å‘½ä»¤è¯´æ˜ï¼š
# 1. è¯·ç¡®ä¿å·²æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ: .venv\\Scripts\\activate
# 2. ç¡®ä¿æ•°æ®é›†è·¯å¾„æ­£ç¡®
# 3. å¾®è°ƒå®Œæˆåï¼Œåˆ·æ–°æ¨¡å‹åˆ—è¡¨å³å¯çœ‹åˆ°æ–°æ¨¡å‹
# 4. æ¨¡å‹å°†ä¿å­˜åˆ°: {model_output}
"""
            return cmd

        start_finetune_btn.click(
            generate_finetune_command,
            inputs=[base_model_select, dataset_path, output_dir, model_name_suffix,
                    epochs, batch_size, learning_rate, device_select],
            outputs=[finetune_output]
        )

        # 9. æ˜¾ç¤ºç®€åŒ–çš„å¾®è°ƒå‘½ä»¤
        def show_finetune_command():
            cmd = """# åŸºæœ¬å¾®è°ƒå‘½ä»¤
python finetune_scripts/finetune_qa.py \\
  --base_model models/qa_models/macbert \\
  --dataset finetune_data/custom_sample \\
  --output_dir finetuned_models/macbert_finetuned \\
  --epochs 3 \\
  --batch_size 8 \\
  --learning_rate 3e-5 \\
  --device auto

# å‡†å¤‡æ•°æ®é›†ç¤ºä¾‹ï¼š
# python src/qa/finetune_data.py
"""
            return cmd

        show_finetune_cmd_btn.click(
            show_finetune_command,
            outputs=[finetune_output]
        )

        # 10. åˆ·æ–°å¾®è°ƒæ¨¡å‹åˆ—è¡¨
        def refresh_finetuned_list():
            import glob
            import os

            finetuned_dir = r"F:\py_work\AMD_AI_Project\AMD_AI_Project\finetuned_models"

            if not os.path.exists(finetuned_dir):
                return "å¾®è°ƒç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»º: finetuned_models/"

            models = glob.glob(f"{finetuned_dir}/*")

            if not models:
                return "æœªæ‰¾åˆ°å¾®è°ƒæ¨¡å‹\n\nè¯·å…ˆè¿è¡Œå¾®è°ƒè®­ç»ƒç”Ÿæˆæ¨¡å‹"

            model_list = "ğŸ“ å·²å‘ç°çš„å¾®è°ƒæ¨¡å‹:\n\n"
            for model_path in models:
                if os.path.isdir(model_path):
                    model_name = os.path.basename(model_path)

                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆæ¨¡å‹
                    required_files = ["config.json", "pytorch_model.bin"]
                    is_valid = all(os.path.exists(os.path.join(model_path, f)) for f in required_files)

                    status = "âœ… æœ‰æ•ˆ" if is_valid else "âš ï¸ ä¸å®Œæ•´"
                    model_list += f"â€¢ {model_name} ({status})\n"

            model_list += f"\nğŸ’¡ å…±å‘ç° {len([m for m in models if os.path.isdir(m)])} ä¸ªæ¨¡å‹"
            model_list += "\nğŸ’¡ åˆ·æ–°é—®ç­”é¡µé¢çš„æ¨¡å‹åˆ—è¡¨å³å¯çœ‹åˆ°è¿™äº›æ¨¡å‹"

            return model_list

        refresh_finetuned_btn.click(
            refresh_finetuned_list,
            outputs=[finetuned_models_list]
        )

        # 11. æ‰“å¼€å¾®è°ƒç›®å½•
        def open_finetune_directory():
            import os
            import subprocess

            finetuned_dir = r"F:\py_work\AMD_AI_Project\AMD_AI_Project\finetuned_models"

            if not os.path.exists(finetuned_dir):
                os.makedirs(finetuned_dir, exist_ok=True)

            try:
                subprocess.Popen(f'explorer "{finetuned_dir}"')
                return f"âœ… å·²æ‰“å¼€å¾®è°ƒç›®å½•:\n{finetuned_dir}"
            except Exception as e:
                return f"âŒ æ‰“å¼€ç›®å½•å¤±è´¥: {str(e)}"

        open_finetune_dir_btn.click(
            open_finetune_directory,
            outputs=[finetune_output]
        )

    return demo


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ å¯åŠ¨æ–‡æ¡£é—®ç­”åŠ©æ‰‹ - ä¼˜åŒ–ç‰ˆ...")
    print("ğŸ’¡ æ–°å¢åŠŸèƒ½:")
    print("  â€¢ æ–‡æœ¬é¢„å¤„ç†å’Œæ¸…æ´—")
    print("  â€¢ é•¿æ–‡æ¡£æ™ºèƒ½åˆ†å—")
    print("  â€¢ ç­”æ¡ˆç½®ä¿¡åº¦é˜ˆå€¼")
    print("  â€¢ ç­”æ¡ˆåå¤„ç†")
    print("  â€¢ ç¤ºä¾‹æ–‡æ¡£å’Œé—®é¢˜")
    print("  â€¢ å†å²è®°å½•å¯¼å‡º")
    print("-" * 50)
    print(f"ğŸ’¡ æœ¬åœ°è®¿é—®: http://127.0.0.1:7861")
    print(f"ğŸŒ å±€åŸŸç½‘è®¿é—®: http://192.168.1.4:7861")
    print("-" * 50)
    print("âš ï¸  å¦‚æœæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·ç¡®ä¿:")
    print("   1. ç½‘ç»œè¿æ¥æ­£å¸¸")
    print("   2. æ¨¡å‹è·¯å¾„æ­£ç¡®")
    print("   3. æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´")

    demo = create_qa_interface()

    demo.launch(
        theme=gr.themes.Soft(),
        server_name="0.0.0.0",
        server_port=7861,
        share=False,
        show_error=True,
        debug=False,
        favicon_path=None
    )


if __name__ == "__main__":
    main()