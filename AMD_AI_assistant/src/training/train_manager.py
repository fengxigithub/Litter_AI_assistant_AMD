#!/usr/bin/env python3
"""
è®­ç»ƒç®¡ç†å™¨ - åŸºäºç°æœ‰é¡¹ç›®ç»“æ„çš„è®­ç»ƒæ¨¡å—
"""

import os
import sys
import json
import torch
import time
import shutil
import subprocess
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# å¯¼å…¥é…ç½®æ–‡ä»¶
try:
    from .config import MODEL_CACHE_DIR, BASE_MODELS
except ImportError:
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å€¼
    MODEL_CACHE_DIR = r"D:\PyCharm Community Edition 2024.1\26.1.22AMD 3.10.19\qianwenchat"


class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨ - ä¸“é—¨ç”¨äºè®­ç»ƒå·²åŠ è½½çš„æ¨¡å‹"""

    def __init__(self, model_manager=None):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            model_manager: å·²æœ‰çš„æ¨¡å‹ç®¡ç†å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        """
        self.model_manager = model_manager
        self.device = None
        self.training_config = self._load_default_config()

        print("=" * 60)
        print("ğŸ¯ AMD 7900XTX æ¨¡å‹è®­ç»ƒå™¨")
        print("=" * 60)

        self._setup_device()

        # åˆå§‹åŒ–æ•°æ®ç›®å½•
        self.data_dir = Path(self.training_config["data_dir"])
        self.data_dir.mkdir(parents=True, exist_ok=True)

    def _convert_data_format(self, item):
        """
        è½¬æ¢ä¸åŒæ ¼å¼çš„æ•°æ®ä¸ºç»Ÿä¸€æ ¼å¼
        Args:
            item: åŸå§‹æ•°æ®é¡¹
        Returns:
            è½¬æ¢åçš„æ•°æ®é¡¹åˆ—è¡¨ï¼ˆå¯èƒ½æœ‰å¤šä¸ªå¯¹è¯è½®æ¬¡ï¼‰
        """
        converted_items = []

        try:
            # æ ¼å¼1: é­”æ­æ•°æ®é›†æ ¼å¼ (æ²é›ªæ•°æ®é›†)
            if 'conversation' in item:
                for conv in item['conversation']:
                    if 'human' in conv and 'assistant' in conv:
                        # å¦‚æœæœ‰systemæŒ‡ä»¤ï¼Œå¯ä»¥æ·»åŠ åˆ°instructionä¸­
                        instruction = conv['human']
                        if 'system' in item:
                            instruction = f"[ç³»ç»ŸæŒ‡ä»¤: {item['system']}] {instruction}"

                        converted_items.append({
                            'instruction': instruction,
                            'response': conv['assistant']
                        })

            # æ ¼å¼2: åŸå§‹æ ¼å¼ (æ‚¨çš„ç¤ºä¾‹æ•°æ®æ ¼å¼)
            elif 'instruction' in item and 'response' in item:
                converted_items.append(item)

            # æ ¼å¼3: å…¶ä»–å¯èƒ½çš„æ ¼å¼ (æ ¹æ®å®é™…æƒ…å†µæ‰©å±•)
            # ä¾‹å¦‚: {'prompt': '...', 'completion': '...'}
            elif 'prompt' in item and 'completion' in item:
                converted_items.append({
                    'instruction': item['prompt'],
                    'response': item['completion']
                })

            # æ ¼å¼4: {'input': '...', 'output': '...'}
            elif 'input' in item and 'output' in item:
                converted_items.append({
                    'instruction': item['input'],
                    'response': item['output']
                })

        except Exception as e:
            print(f"âš ï¸  æ•°æ®æ ¼å¼è½¬æ¢å¤±è´¥: {e}")

        return converted_items


    def _load_default_config(self):
        """åŠ è½½é»˜è®¤è®­ç»ƒé…ç½®"""
        return {
            "base_model": "Qwen/Qwen2.5-0.5B-Instruct",
            "cache_dir": MODEL_CACHE_DIR,
            "output_dir": str(project_root / "models/trained"),
            "data_dir": str(project_root / "data"),
            "epochs": 5,  # å¢åŠ è®­ç»ƒè½®æ•°
            "batch_size": 1,  # å‡å°æ‰¹å¤§å°ï¼ˆDirectMLå†…å­˜é™åˆ¶ï¼‰
            "learning_rate": 2e-4,  # è°ƒæ•´å­¦ä¹ ç‡
            "max_length": 256,  # å‡å°åºåˆ—é•¿åº¦
            "logging_steps": 1,  # å¢åŠ æ—¥å¿—é¢‘ç‡
            "save_steps": 50,
            "warmup_steps": 10,
            "gradient_accumulation_steps": 8,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯
            "fp16": False,
            "gradient_checkpointing": True,
            "lr_scheduler_type": "cosine",  # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨
            "weight_decay": 0.01,  # æ·»åŠ æƒé‡è¡°å‡
        }

    def _setup_device(self):
        """è®¾ç½®è®­ç»ƒè®¾å¤‡ - ä¼˜åŒ–ç‰ˆ"""
        try:
            import torch_directml
            self.device = torch_directml.device()
            print(f"âœ… è®­ç»ƒè®¾å¤‡: DirectML ({self.device})")

            # DirectMLç‰¹å®šé…ç½®
            self.training_config.update({
                "fp16": False,
                "batch_size": 1,  # DirectMLé€šå¸¸éœ€è¦è¾ƒå°æ‰¹æ¬¡
                "gradient_accumulation_steps": 8,
                "dataloader_pin_memory": False,
                "dataloader_num_workers": 0,
            })
            print("ğŸ¯ å·²åº”ç”¨DirectMLä¼˜åŒ–é…ç½®")

        except ImportError:
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
                print(f"âœ… è®­ç»ƒè®¾å¤‡: CUDA")
            else:
                self.device = torch.device("cpu")
                print("âš ï¸  è®­ç»ƒè®¾å¤‡: CPUï¼ˆè®­ç»ƒä¼šå¾ˆæ…¢ï¼‰")

    def prepare_training_data(self, data_type="example", data_path=None):
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼Œæ”¯æŒé­”æ­æ•°æ®é›†ï¼‰

        Args:
            data_type: æ•°æ®ç±»å‹ - "example"ï¼ˆç¤ºä¾‹ï¼‰, "file"ï¼ˆæ–‡ä»¶ï¼‰,
                      "manual"ï¼ˆæ‰‹åŠ¨è¾“å…¥ï¼‰, "modelscope"ï¼ˆé­”æ­æ•°æ®é›†ï¼‰
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå½“data_type="file"æ—¶ä½¿ç”¨ï¼‰

        Returns:
            data_path: å‡†å¤‡å¥½çš„æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        print("\nğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")

        # å¦‚æœç”¨æˆ·é€‰æ‹©é­”æ­æ•°æ®é›†
        if data_type == "modelscope":
            return self.download_modelscope_dataset()

        # åŸæœ‰çš„æ•°æ®å¤„ç†é€»è¾‘
        data_dir = Path(self.training_config["data_dir"])
        data_dir.mkdir(parents=True, exist_ok=True)

        if data_type == "example":
            # åˆ›å»ºç¤ºä¾‹æ•°æ®
            data_path = data_dir / "example_training_data.jsonl"
            self._create_example_data(data_path)

        elif data_type == "file" and data_path:
            if not Path(data_path).exists():
                print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
                return None
            # éªŒè¯æ•°æ®æ ¼å¼
            if not self._validate_data_file(data_path):
                return None

        elif data_type == "manual":
            data_path = data_dir / "manual_training_data.jsonl"
            self._collect_manual_data(data_path)

        else:
            print("âŒ ä¸æ”¯æŒçš„æ•°æ®ç±»å‹")
            return None

        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {data_path}")
        return str(data_path)

    def _create_example_data(self, data_path, num_samples=20):
        """åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ•°æ®"""
        example_data = [
            {
                "instruction": "ä»‹ç»ä¸€ä¸‹Pythonè¯­è¨€",
                "response": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumäº1991å¹´åˆ›å»ºã€‚å®ƒå…·æœ‰ç®€æ´æ˜“è¯»çš„è¯­æ³•ï¼Œå¹¿æ³›åº”ç”¨äºWebå¼€å‘ã€æ•°æ®åˆ†æã€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚"
            },
            {
                "instruction": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
                "response": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºé¢„æµ‹æˆ–å†³ç­–ï¼Œè€Œæ— éœ€æ˜¾å¼ç¼–ç¨‹ã€‚"
            },
            {
                "instruction": "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
                "response": "å­¦ä¹ ç¼–ç¨‹å¯ä»¥ä»åŸºç¡€å¼€å§‹ï¼š1. é€‰æ‹©ä¸€é—¨è¯­è¨€ï¼ˆå¦‚Pythonï¼‰2. å­¦ä¹ åŸºæœ¬è¯­æ³• 3. åšå®é™…é¡¹ç›® 4. é˜…è¯»ä¼˜ç§€ä»£ç  5. å‚ä¸å¼€æºé¡¹ç›®ã€‚"
            },
            {
                "instruction": "AMDæ˜¾å¡æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
                "response": "AMDæ˜¾å¡çš„ä¼˜åŠ¿åŒ…æ‹¬ï¼š1. æ€§ä»·æ¯”é«˜ 2. æ”¯æŒå¼€æºé©±åŠ¨ 3. åœ¨ä¸“ä¸šè®¡ç®—å’ŒAIæ–¹é¢è¡¨ç°è‰¯å¥½ 4. æ”¯æŒDirectMLåŠ é€Ÿã€‚"
            },
            {
                "instruction": "ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ",
                "response": "ç¥ç»ç½‘ç»œæ˜¯å—ç”Ÿç‰©ç¥ç»ç½‘ç»œå¯å‘çš„è®¡ç®—æ¨¡å‹ï¼Œç”±ç›¸äº’è¿æ¥çš„èŠ‚ç‚¹ï¼ˆç¥ç»å…ƒï¼‰ç»„æˆï¼Œç”¨äºæ¨¡å¼è¯†åˆ«å’Œæœºå™¨å­¦ä¹ ä»»åŠ¡ã€‚"
            },
            {
                "instruction": "ä»‹ç»ä¸€ä¸‹Pythonè¯­è¨€çš„ç‰¹ç‚¹å’Œåº”ç”¨åœºæ™¯",
                "response": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š1. ç®€æ´æ˜“è¯»çš„è¯­æ³• 2. è·¨å¹³å°æ€§ 3. ä¸°å¯Œçš„ç¬¬ä¸‰æ–¹åº“ 4. å¼ºå¤§çš„ç¤¾åŒºæ”¯æŒã€‚ä¸»è¦åº”ç”¨åœºæ™¯åŒ…æ‹¬ï¼šWebå¼€å‘ã€æ•°æ®åˆ†æã€äººå·¥æ™ºèƒ½ã€ç§‘å­¦è®¡ç®—ã€è‡ªåŠ¨åŒ–è„šæœ¬ç­‰ã€‚"
            }
        ]

        # å¤åˆ¶æ•°æ®ä»¥è¾¾åˆ°æŒ‡å®šæ•°é‡
        all_data = []
        for i in range(num_samples):
            data = example_data[i % len(example_data)].copy()
            data["response"] = f"[æ ·æœ¬{i + 1}] " + data["response"]
            all_data.append(data)

        # ä¿å­˜ä¸ºJSONLæ ¼å¼
        with open(data_path, 'w', encoding='utf-8') as f:
            for item in all_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')

        print(f"ğŸ“ åˆ›å»ºäº† {len(all_data)} æ¡ç¤ºä¾‹æ•°æ®")

    def _validate_data_file(self, data_path):
        """éªŒè¯æ•°æ®æ–‡ä»¶æ ¼å¼"""
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            valid_count = 0
            for line in lines:
                try:
                    data = json.loads(line.strip())
                    if 'instruction' in data and 'response' in data:
                        valid_count += 1
                except:
                    continue

            print(f"ğŸ“Š æ•°æ®æ–‡ä»¶éªŒè¯: {valid_count}/{len(lines)} æ¡æœ‰æ•ˆæ•°æ®")
            return valid_count > 0

        except Exception as e:
            print(f"âŒ æ•°æ®æ–‡ä»¶éªŒè¯å¤±è´¥: {e}")
            return False

    def _collect_manual_data(self, data_path):
        """æ‰‹åŠ¨æ”¶é›†è®­ç»ƒæ•°æ®"""
        print("ğŸ“ æ‰‹åŠ¨è¾“å…¥è®­ç»ƒæ•°æ®ï¼ˆè¾“å…¥ç©ºè¡Œç»“æŸï¼‰")
        print("æ ¼å¼: é—®é¢˜ \\n ç­”æ¡ˆ")
        print("-" * 40)

        data = []
        while True:
            print(f"\næ ·æœ¬ #{len(data) + 1}")
            instruction = input("é—®é¢˜: ").strip()
            if not instruction:
                break

            response = input("ç­”æ¡ˆ: ").strip()
            if not response:
                print("âš ï¸  ç­”æ¡ˆä¸èƒ½ä¸ºç©ºï¼Œè·³è¿‡æ­¤æ ·æœ¬")
                continue

            data.append({
                "instruction": instruction,
                "response": response
            })

            more = input("ç»§ç»­è¾“å…¥ï¼Ÿ(y/n): ").strip().lower()
            if more != 'y':
                break

        if data:
            with open(data_path, 'w', encoding='utf-8') as f:
                for item in data:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
            print(f"âœ… ä¿å­˜äº† {len(data)} æ¡è®­ç»ƒæ•°æ®")
        else:
            print("âš ï¸  æœªè¾“å…¥ä»»ä½•æ•°æ®")

    def download_modelscope_dataset(self):
        """
        ä»é­”æ­ç¤¾åŒºä¸‹è½½æ•°æ®é›†

        Returns:
            data_path: ä¸‹è½½çš„æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        """
        print("\n" + "=" * 60)
        print("ğŸŒ é­”æ­ç¤¾åŒºæ•°æ®é›†ä¸‹è½½")
        print("=" * 60)

        # è¯¢é—®ç”¨æˆ·æ•°æ®é›†è·¯å¾„
        print("\nğŸ“¥ è¯·è¾“å…¥é­”æ­ç¤¾åŒºçš„æ•°æ®é›†è·¯å¾„:")
        print("æ ¼å¼ç¤ºä¾‹: Moemuu/Muice-Dataset")
        print("          damo/æ•°æ®é›†å")
        print("          namespace/dataset_name")
        print("\næ‚¨å¯ä»¥åœ¨é­”æ­æ•°æ®é›†é¡µé¢æ‰¾åˆ°è¿™ä¸ªè·¯å¾„")
        dataset_path = input("æ•°æ®é›†è·¯å¾„: ").strip()

        if not dataset_path:
            print("âŒ æœªè¾“å…¥æ•°æ®é›†è·¯å¾„")
            return None

        # æå–æ•°æ®é›†åç§°ï¼ˆç”¨äºæœ¬åœ°æ–‡ä»¶å¤¹å‘½åï¼‰
        if '/' in dataset_path:
            dataset_name = dataset_path.split('/')[-1]
        else:
            dataset_name = dataset_path

        # æœ¬åœ°ä¿å­˜è·¯å¾„
        local_dataset_dir = self.data_dir / f"modelscope_{dataset_name}"

        # æ£€æŸ¥æ˜¯å¦å·²ç»ä¸‹è½½
        if local_dataset_dir.exists():
            print(f"ğŸ“ æ£€æµ‹åˆ°å·²ä¸‹è½½çš„æ•°æ®é›†: {local_dataset_dir}")

            # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒæ•°æ®æ–‡ä»¶
            train_files = list(local_dataset_dir.glob("*train*"))
            if train_files:
                print(f"âœ… ä½¿ç”¨å·²ä¸‹è½½çš„æ•°æ®é›†ï¼ˆè·³è¿‡ä¸‹è½½ï¼‰")
                train_file = self._find_training_file(local_dataset_dir)
                if train_file:
                    return str(train_file)
            else:
                print("âš ï¸  æ•°æ®é›†æ–‡ä»¶å¤¹å­˜åœ¨ä½†æ²¡æœ‰è®­ç»ƒæ–‡ä»¶ï¼Œé‡æ–°ä¸‹è½½...")
                try:
                    shutil.rmtree(local_dataset_dir)
                except:
                    pass

        print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½æ•°æ®é›†: {dataset_path}")
        print(f"ğŸ“ ä¿å­˜åˆ°: {local_dataset_dir}")

        try:
            # æ–¹æ³•1: ä½¿ç”¨modelscopeçš„Python APIï¼ˆæ¨èï¼‰
            try:
                from modelscope.msdatasets import MsDataset

                print("ğŸ”§ ä½¿ç”¨ModelScope APIä¸‹è½½...")

                # ä¸‹è½½æ•°æ®é›†
                dataset = MsDataset.load(
                    dataset_path,
                    subset_name=None,  # å¦‚æœæœ‰å­é›†å¯ä»¥æŒ‡å®š
                    split=None,  # ä¸‹è½½æ‰€æœ‰åˆ†å‰²
                    cache_dir=str(local_dataset_dir),
                    download_mode="force_redownload"
                )

                # ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
                local_dataset_dir.mkdir(parents=True, exist_ok=True)

                # å¤„ç†ä¸‹è½½çš„æ•°æ®é›†
                train_file = self._process_downloaded_dataset(dataset, local_dataset_dir)

                if train_file:
                    print(f"âœ… æ•°æ®é›†ä¸‹è½½å®Œæˆ: {train_file}")
                    return str(train_file)
                else:
                    print("âŒ æ— æ³•å¤„ç†ä¸‹è½½çš„æ•°æ®é›†")
                    return None

            except ImportError:
                print("âš ï¸  modelscopeåº“æœªå®‰è£…ï¼Œä½¿ç”¨å‘½ä»¤è¡Œä¸‹è½½...")
                # æ–¹æ³•2: ä½¿ç”¨å‘½ä»¤è¡Œä¸‹è½½
                return self._download_via_commandline(dataset_path, local_dataset_dir)

        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _process_downloaded_dataset(self, dataset, output_dir):
        """å¤„ç†ä¸‹è½½çš„æ•°æ®é›†ï¼Œè½¬æ¢ä¸ºJSONLæ ¼å¼"""
        output_dir = Path(output_dir)

        # å¦‚æœæ˜¯å¤šæ–‡ä»¶æ•°æ®é›†ï¼Œdatasetå¯èƒ½æ˜¯å­—å…¸
        if isinstance(dataset, dict):
            for split_name, split_data in dataset.items():
                print(f"ğŸ“Š å¤„ç†åˆ†å‰²: {split_name}")

                # ä¿å­˜ä¸ºJSONLæ ¼å¼
                split_file = output_dir / f"{split_name}.jsonl"
                self._save_dataset_as_jsonl(split_data, split_file)
        else:
            # å•ä¸ªæ•°æ®é›†
            train_file = output_dir / "train.jsonl"
            self._save_dataset_as_jsonl(dataset, train_file)

        # æŸ¥æ‰¾è®­ç»ƒæ–‡ä»¶
        train_file = self._find_training_file(output_dir)
        return train_file

    def _save_dataset_as_jsonl(self, dataset, output_file):
        """å°†æ•°æ®é›†ä¿å­˜ä¸ºJSONLæ ¼å¼"""
        print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_file}")

        count = 0
        with open(output_file, 'w', encoding='utf-8') as f:
            try:
                # å°è¯•ä¸åŒçš„æ•°æ®é›†è®¿é—®æ–¹å¼
                if hasattr(dataset, '_hf_ds'):
                    # å¦‚æœæ˜¯MsDatasetåŒ…è£…çš„HuggingFaceæ•°æ®é›†
                    hf_ds = dataset._hf_ds
                    for item in hf_ds:
                        f.write(json.dumps(dict(item), ensure_ascii=False) + '\n')
                        count += 1
                elif hasattr(dataset, '__iter__'):
                    # å¦‚æœæ˜¯å¯è¿­ä»£å¯¹è±¡
                    for item in dataset:
                        f.write(json.dumps(dict(item), ensure_ascii=False) + '\n')
                        count += 1
                else:
                    print(f"âš ï¸  æœªçŸ¥çš„æ•°æ®é›†ç±»å‹: {type(dataset)}")
            except Exception as e:
                print(f"âš ï¸  ä¿å­˜æ•°æ®é›†æ—¶å‡ºé”™: {e}")

        print(f"ğŸ“ ä¿å­˜äº† {count} æ¡æ•°æ®")

    def _download_via_commandline(self, dataset_path, local_dir):
        """é€šè¿‡å‘½ä»¤è¡Œä¸‹è½½æ•°æ®é›†"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            local_dir = Path(local_dir)
            local_dir.mkdir(parents=True, exist_ok=True)

            print("ğŸ”„ ä½¿ç”¨å‘½ä»¤è¡Œä¸‹è½½...")

            # æ„å»ºä¸‹è½½å‘½ä»¤
            cmd = [
                "modelscope",
                "download",
                "--dataset",
                dataset_path,
                "--local_dir",
                str(local_dir)
            ]

            print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")

            # æ‰§è¡Œä¸‹è½½
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                encoding='utf-8'
            )

            if result.returncode == 0:
                print("âœ… å‘½ä»¤è¡Œä¸‹è½½æˆåŠŸ")

                # æŸ¥æ‰¾è®­ç»ƒæ–‡ä»¶
                train_file = self._find_training_file(local_dir)
                if train_file:
                    return str(train_file)
                else:
                    print("âŒ ä¸‹è½½æˆåŠŸä½†æœªæ‰¾åˆ°è®­ç»ƒæ–‡ä»¶")
                    return None
            else:
                print(f"âŒ å‘½ä»¤è¡Œä¸‹è½½å¤±è´¥")
                print(f"é”™è¯¯: {result.stderr}")
                return None

        except Exception as e:
            print(f"âŒ å‘½ä»¤è¡Œä¸‹è½½å¼‚å¸¸: {e}")
            return None

    def _find_training_file(self, dataset_dir):
        """åœ¨æ•°æ®é›†ç›®å½•ä¸­æŸ¥æ‰¾è®­ç»ƒæ–‡ä»¶"""
        dataset_dir = Path(dataset_dir)

        if not dataset_dir.exists():
            return None

        # ä¼˜å…ˆçº§æœç´¢æ¨¡å¼
        search_patterns = [
            "train.jsonl",
            "train.json",
            "*train*.jsonl",
            "*train*.json",
            "train.csv",
            "data.jsonl",
            "dataset.jsonl",
        ]

        for pattern in search_patterns:
            files = list(dataset_dir.glob(pattern))
            if files:
                return files[0]

        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ªJSON/JSONLæ–‡ä»¶
        for ext in ['.jsonl', '.json', '.csv']:
            files = list(dataset_dir.glob(f"*{ext}"))
            if files:
                return files[0]

        return None

    # ä»¥ä¸‹æ˜¯åŸæœ‰è®­ç»ƒåŠŸèƒ½ï¼Œä¿æŒä¸å˜
    def train_full_model(self, model_path=None, data_path=None, config=None, force_base_model=False): #force_base_model=Falseè¿™é‡Œå¯é€‰æ˜¯å¦ç”¨åŸºç¡€æ¨¡å‹ï¼Œmodel_path=Noneè¿™é‡Œæ˜¯é€‰æ‹©æ¨¡å‹è·¯å¾„ï¼Œæ–‡ä»¶å¤¹å³å¯
        """
        å…¨å‚æ•°å¾®è°ƒè®­ç»ƒ - ä¿®å¤ç‰ˆ
        """
        print("\n" + "=" * 60)
        print("ğŸš€ å¼€å§‹å…¨å‚æ•°å¾®è°ƒè®­ç»ƒ")
        print("=" * 60)

        # æ›´æ–°é…ç½®
        if config:
            self.training_config.update(config)

        # å‡†å¤‡æ•°æ®
        if not data_path:
            data_path = self.prepare_training_data("example")
            if not data_path:
                return None

        try:
            # åŠ¨æ€å¯¼å…¥transformers
            from transformers import (
                AutoTokenizer,
                AutoModelForCausalLM,
                TrainingArguments,
                Trainer,
                DataCollatorForLanguageModeling
            )
            from datasets import Dataset

            # ğŸ”§ æ™ºèƒ½é€‰æ‹©æ¨¡å‹è·¯å¾„
            if force_base_model:
                load_path = self.training_config["base_model"]
                print(f"ğŸ“¦ å¼ºåˆ¶ä½¿ç”¨åŸºç¡€æ¨¡å‹: {load_path}")
            elif model_path:
                load_path = model_path
                print(f"ğŸ“ ä½¿ç”¨æŒ‡å®šæ¨¡å‹: {load_path}")
            else:
                # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„è®­ç»ƒæ¨¡å‹
                models_dir = Path(r"F:\py_work\AMD_AI_Project\AMD_AI_Project\models\trained")
                latest_model = None

                if models_dir.exists():
                    training_dirs = []
                    for item in models_dir.iterdir():
                        if item.is_dir() and (item / "final_model").exists():
                            training_dirs.append(item)

                    if training_dirs:
                        training_dirs.sort(key=lambda x: x.name, reverse=True)
                        latest_model = training_dirs[0] / "final_model"
                        load_path = str(latest_model)
                        print(f"âœ… è‡ªåŠ¨é€‰æ‹©æœ€æ–°è®­ç»ƒæ¨¡å‹: {latest_model.parent.name}")
                    else:
                        load_path = self.training_config["base_model"]
                        print(f"ğŸ“¦ ä½¿ç”¨åŸºç¡€æ¨¡å‹: {load_path}")
                else:
                    load_path = self.training_config["base_model"]
                    print(f"ğŸ“¦ ä½¿ç”¨åŸºç¡€æ¨¡å‹: {load_path}")

            # åˆ¤æ–­æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„
            is_local_path = Path(load_path).exists()
            print(f"ğŸ“Š æ¨¡å‹ç±»å‹: {'æœ¬åœ°æ¨¡å‹' if is_local_path else 'HuggingFaceæ¨¡å‹'}")

            # 1. åŠ è½½tokenizer
            print(f"\nğŸ”§ åŠ è½½tokenizer...")
            cache_dir = r"D:\PyCharm Community Edition 2024.1\26.1.22AMD 3.10.19\qianwenchat"

            if is_local_path:
                tokenizer = AutoTokenizer.from_pretrained(
                    load_path,
                    trust_remote_code=True,
                    local_files_only=True
                )
            else:
                tokenizer = AutoTokenizer.from_pretrained(
                    load_path,
                    cache_dir=cache_dir,
                    trust_remote_code=True,
                    local_files_only=True
                )

            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            # 2. åŠ è½½æ¨¡å‹
            print("ğŸ”§ åŠ è½½æ¨¡å‹...")
            if is_local_path:
                model = AutoModelForCausalLM.from_pretrained(
                    load_path,
                    torch_dtype=torch.float32,
                    trust_remote_code=True,
                    local_files_only=True
                ).to(self.device)
            else:
                model = AutoModelForCausalLM.from_pretrained(
                    load_path,
                    cache_dir=cache_dir,
                    torch_dtype=torch.float32,
                    trust_remote_code=True,
                    local_files_only=True
                ).to(self.device)

            # 3. å‡†å¤‡æ•°æ®é›† - ä¿®å¤ç‰ˆ
            print("ğŸ“Š å‡†å¤‡æ•°æ®é›†...")

            # ä¿®æ”¹åçš„ä»£ç ï¼š
            data = []
            with open(data_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        item = json.loads(line.strip())

                        # å¤„ç†é­”æ­æ•°æ®é›†æ ¼å¼
                        if 'conversation' in item:
                            # ä»conversationä¸­æå–instructionå’Œresponse
                            for conv in item['conversation']:
                                if 'human' in conv and 'assistant' in conv:
                                    data.append({
                                        'instruction': conv['human'],
                                        'response': conv['assistant']
                                    })
                        # å¤„ç†åŸå§‹æ ¼å¼
                        elif 'instruction' in item and 'response' in item:
                            data.append(item)
                    except Exception as e:
                        print(f"âš ï¸  è§£ææ•°æ®è¡Œå¤±è´¥: {e}")
                        continue

            print(f"ğŸ“ˆ åŠ è½½ {len(data)} æ¡è®­ç»ƒæ•°æ®")

            # æ·»åŠ æ•°æ®éªŒè¯
            if len(data) == 0:
                print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆè®­ç»ƒæ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼")
                print("ğŸ’¡ æ•°æ®æ ¼å¼åº”è¯¥æ˜¯ï¼š")
                print("  æ ¼å¼1: {'instruction': '...', 'response': '...'}")
                print("  æ ¼å¼2: {'conversation': [{'human': '...', 'assistant': '...'}]}")
                return None

            # Qwenå¯¹è¯æ ¼å¼
            def format_example(example):
                messages = [
                    {"role": "user", "content": example['instruction']},
                    {"role": "assistant", "content": example['response']}
                ]
                formatted = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=False
                )
                return formatted

            formatted_texts = [format_example(item) for item in data]

            # åˆ›å»ºDataset
            full_dataset = Dataset.from_dict({"text": formatted_texts})

            # åˆ†å‰²æ•°æ®é›†
            split_dataset = full_dataset.train_test_split(
                test_size=0.1,  # 10%ä½œä¸ºéªŒè¯é›†
                shuffle=True,
                seed=42
            )

            train_raw_dataset = split_dataset["train"]
            eval_raw_dataset = split_dataset["test"]

            print(f"ğŸ“Š è®­ç»ƒé›†: {len(train_raw_dataset)} æ¡ï¼ŒéªŒè¯é›†: {len(eval_raw_dataset)} æ¡")

            # åˆ†è¯å‡½æ•°
            def tokenize_function(examples):
                return tokenizer(
                    examples["text"],
                    truncation=True,
                    max_length=self.training_config["max_length"],
                    padding=False
                )

            # å¯¹è®­ç»ƒé›†å’ŒéªŒè¯é›†è¿›è¡Œåˆ†è¯
            train_dataset = train_raw_dataset.map(
                tokenize_function,
                batched=True,
                remove_columns=["text"]
            )

            eval_dataset = eval_raw_dataset.map(
                tokenize_function,
                batched=True,
                remove_columns=["text"]
            )

            # 4. è®­ç»ƒå‚æ•°
            output_dir = Path(self.training_config["output_dir"]) / datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir.mkdir(parents=True, exist_ok=True)

            training_args = TrainingArguments(
                output_dir=str(output_dir),
                num_train_epochs=self.training_config["epochs"],
                per_device_train_batch_size=self.training_config["batch_size"],
                per_device_eval_batch_size=1,
                gradient_accumulation_steps=self.training_config["gradient_accumulation_steps"],
                warmup_steps=self.training_config["warmup_steps"],
                logging_steps=self.training_config["logging_steps"],
                save_steps=self.training_config["save_steps"],
                evaluation_strategy="steps",
                eval_steps=50,
                learning_rate=self.training_config["learning_rate"],
                lr_scheduler_type="cosine",
                weight_decay=0.01,
                fp16=self.training_config["fp16"],
                gradient_checkpointing=self.training_config["gradient_checkpointing"],
                optim="adamw_torch",
                report_to="none",
                save_total_limit=2,
                remove_unused_columns=False,
                logging_first_step=True,
                gradient_checkpointing_kwargs={"use_reentrant": False},
                dataloader_pin_memory=False,
                dataloader_num_workers=0,
                load_best_model_at_end=True,
                metric_for_best_model="eval_loss",
            )

            # 5. æ•°æ®æ•´ç†å™¨
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=tokenizer,
                mlm=False,
                pad_to_multiple_of=8,
            )

            # 6. åˆ›å»ºTrainer
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                data_collator=data_collator,
                tokenizer=tokenizer,
            )

            # 7. å¼€å§‹è®­ç»ƒ
            print("\nğŸ”¥ å¼€å§‹è®­ç»ƒ...")
            print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
            print(f"  â€¢ åŸºç¡€æ¨¡å‹: {Path(load_path).name if is_local_path else load_path}")
            print(f"  â€¢ æ•°æ®é‡: {len(train_dataset)} æ¡")
            print(f"  â€¢ è®­ç»ƒè½®æ•°: {self.training_config['epochs']}")
            print(f"  â€¢ æ‰¹å¤§å°: {self.training_config['batch_size']}")
            print(f"  â€¢ å­¦ä¹ ç‡: {self.training_config['learning_rate']}")
            print(f"  â€¢ è¾“å‡ºç›®å½•: {output_dir}")
            print("-" * 40)

            start_time = time.time()
            trainer.train()
            training_time = time.time() - start_time

            # 8. ä¿å­˜æ¨¡å‹
            final_model_dir = output_dir / "final_model"
            trainer.save_model(str(final_model_dir))
            tokenizer.save_pretrained(str(final_model_dir))

            print(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
            print(f"â±ï¸  è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
            print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {final_model_dir}")

            return str(final_model_dir)

        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    def train_lora(self, model_path=None, data_path=None, config=None):
        """
        LoRAå¾®è°ƒè®­ç»ƒï¼ˆæ˜¾å­˜è¦æ±‚ä½ï¼‰

        Args:
            model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
            data_path: è®­ç»ƒæ•°æ®è·¯å¾„
            config: LoRAé…ç½®

        Returns:
            lora_weights_path: LoRAæƒé‡è·¯å¾„
        """
        print("\n" + "=" * 60)
        print("ğŸ¯ å¼€å§‹LoRAå¾®è°ƒè®­ç»ƒ")
        print("=" * 60)

        try:
            # åŠ¨æ€å¯¼å…¥
            from transformers import (
                AutoTokenizer,
                AutoModelForCausalLM,
                TrainingArguments,
                Trainer,
                DataCollatorForLanguageModeling,
            )

            # æ£€æŸ¥æ˜¯å¦å®‰è£…äº†peft
            try:
                from peft import LoraConfig, get_peft_model, TaskType
            except ImportError:
                print("âŒ éœ€è¦å®‰è£…peftåº“: pip install peft")
                return None

            # é»˜è®¤LoRAé…ç½®
            lora_config = {
                "r": 8,
                "lora_alpha": 32,
                "lora_dropout": 0.1,
                "target_modules": ["q_proj", "v_proj"],
                "bias": "none",
            }

            # æ›´æ–°ç”¨æˆ·é…ç½®
            if config:
                for key in ["r", "lora_alpha", "lora_dropout", "target_modules"]:
                    if key in config:
                        lora_config[key] = config[key]

            # 1. æ™ºèƒ½é€‰æ‹©æ¨¡å‹è·¯å¾„
            if model_path:
                load_path = model_path
            else:
                models_dir = Path(r"F:\py_work\AMD_AI_Project\AMD_AI_Project\models\trained")

                latest_model = None
                if models_dir.exists():
                    training_dirs = []
                    for item in models_dir.iterdir():
                        if item.is_dir() and (item / "final_model").exists():
                            if len(item.name) == 15 and item.name[:8].isdigit():
                                training_dirs.append(item)

                    if training_dirs:
                        training_dirs.sort(key=lambda x: x.name, reverse=True)
                        latest_model = training_dirs[0] / "final_model"
                        load_path = str(latest_model)
                        print(f"âœ… ä½¿ç”¨æœ€æ–°è®­ç»ƒæ¨¡å‹: {latest_model.parent.name}")
                    else:
                        load_path = "Qwen/Qwen2.5-0.5B-Instruct"
                        print(f"ğŸ“¦ ä½¿ç”¨åŸºç¡€æ¨¡å‹: {load_path}")
                else:
                    load_path = "Qwen/Qwen2.5-0.5B-Instruct"
                    print(f"ğŸ“¦ ä½¿ç”¨åŸºç¡€æ¨¡å‹: {load_path}")

            # åˆ¤æ–­æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„
            is_local_path = Path(load_path).exists()
            print(f"ğŸ“Š æ¨¡å‹ç±»å‹: {'æœ¬åœ°æ¨¡å‹' if is_local_path else 'HuggingFaceæ¨¡å‹'}")

            # 2. åŠ è½½tokenizer
            print("ğŸ”§ åŠ è½½tokenizer...")
            cache_dir = r"D:\PyCharm Community Edition 2024.1\26.1.22AMD 3.10.19\qianwenchat"

            if is_local_path:
                tokenizer = AutoTokenizer.from_pretrained(
                    load_path,
                    trust_remote_code=True,
                    local_files_only=True
                )
            else:
                tokenizer = AutoTokenizer.from_pretrained(
                    load_path,
                    cache_dir=cache_dir,
                    trust_remote_code=True,
                    local_files_only=True
                )

            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            # 3. åŠ è½½æ¨¡å‹
            print("ğŸ”§ åŠ è½½æ¨¡å‹...")
            try:
                # å°è¯•é‡åŒ–åŠ è½½
                try:
                    from transformers import BitsAndBytesConfig
                    bnb_config = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=torch.float16,
                        bnb_4bit_quant_type="nf4",
                        bnb_4bit_use_double_quant=True
                    )

                    model = AutoModelForCausalLM.from_pretrained(
                        load_path,
                        quantization_config=bnb_config,
                        trust_remote_code=True,
                        device_map="auto",
                        cache_dir=cache_dir if not is_local_path else None
                    )
                    print("âœ… ä½¿ç”¨4ä½é‡åŒ–åŠ è½½")

                except Exception as e:
                    print(f"âš ï¸  é‡åŒ–åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šç²¾åº¦: {e}")
                    if is_local_path:
                        model = AutoModelForCausalLM.from_pretrained(
                            load_path,
                            torch_dtype=torch.float32,
                            trust_remote_code=True,
                            local_files_only=True
                        ).to(self.device)
                    else:
                        model = AutoModelForCausalLM.from_pretrained(
                            load_path,
                            cache_dir=cache_dir,
                            torch_dtype=torch.float32,
                            trust_remote_code=True,
                            local_files_only=True
                        ).to(self.device)

            except Exception as e:
                print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
                return None

            # 4. åº”ç”¨LoRA
            print("ğŸ›ï¸  åº”ç”¨LoRAé…ç½®...")
            peft_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                **lora_config
            )

            model = get_peft_model(model, peft_config)
            model.print_trainable_parameters()

            # 5. å‡†å¤‡æ•°æ®
            print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
            if not data_path:
                print("âŒ æœªæä¾›è®­ç»ƒæ•°æ®")
                return None

            dataset = self._prepare_dataset(tokenizer, data_path)

            # 6. è®­ç»ƒå‚æ•°
            output_dir = Path(self.training_config["output_dir"]) / f"lora_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            output_dir.mkdir(parents=True, exist_ok=True)

            training_args = TrainingArguments(
                output_dir=str(output_dir),
                num_train_epochs=config.get("epochs", 3) if config else 3,
                per_device_train_batch_size=config.get("batch_size", 4) if config else 4,
                gradient_accumulation_steps=config.get("gradient_accumulation_steps", 4) if config else 4,
                warmup_steps=config.get("warmup_steps", 50) if config else 50,
                logging_steps=config.get("logging_steps", 10) if config else 10,
                save_steps=config.get("save_steps", 50) if config else 50,
                learning_rate=config.get("learning_rate", 2e-4) if config else 2e-4,
                fp16=self.training_config.get("fp16", False),
                gradient_checkpointing=True,
                optim="adamw_torch",
                report_to="none",
                save_total_limit=2,
                remove_unused_columns=False,
            )

            # 7. æ•°æ®æ•´ç†å™¨
            # ä¿®æ”¹DataCollator
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=tokenizer,
                mlm=False,
                pad_to_multiple_of=8,  # æ·»åŠ è¿™ä¸ªï¼Œå¯èƒ½æœ‰åŠ©äºDirectMLæ€§èƒ½
            )

            # 8. åˆ›å»ºTrainer
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=dataset,
                data_collator=data_collator,
            )

            # 9. å¼€å§‹è®­ç»ƒ
            print("\nğŸ”¥ å¼€å§‹LoRAè®­ç»ƒ...")
            print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
            print(f"  â€¢ åŸºç¡€æ¨¡å‹: {Path(load_path).name if is_local_path else load_path}")
            print(f"  â€¢ æ•°æ®é‡: {len(dataset)} æ¡")
            print(f"  â€¢ è®­ç»ƒè½®æ•°: {training_args.num_train_epochs}")
            print(f"  â€¢ æ‰¹å¤§å°: {training_args.per_device_train_batch_size}")
            print(f"  â€¢ å­¦ä¹ ç‡: {training_args.learning_rate}")
            print(f"  â€¢ LoRAé…ç½®: r={lora_config['r']}, alpha={lora_config['lora_alpha']}")
            print(f"  â€¢ è¾“å‡ºç›®å½•: {output_dir}")
            print("-" * 40)

            start_time = time.time()
            trainer.train()
            training_time = time.time() - start_time

            # 10. ä¿å­˜LoRAæƒé‡
            lora_dir = output_dir / "lora_weights"
            model.save_pretrained(str(lora_dir))
            tokenizer.save_pretrained(str(lora_dir))

            print(f"\nâœ… LoRAè®­ç»ƒå®Œæˆï¼")
            print(f"â±ï¸  è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
            print(f"ğŸ“ LoRAæƒé‡ä¿å­˜åœ¨: {lora_dir}")

            return str(lora_dir)

        except Exception as e:
            print(f"âŒ LoRAè®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _prepare_dataset(self, tokenizer, data_path):
        """å‡†å¤‡æ•°æ®é›† - ä¿®å¤ç‰ˆæœ¬"""
        # åŠ è½½æ•°æ®
        # ä¿®æ”¹åçš„ä»£ç ï¼š
        data = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    item = json.loads(line.strip())

                    # å¤„ç†é­”æ­æ•°æ®é›†æ ¼å¼
                    if 'conversation' in item:
                        # ä»conversationä¸­æå–instructionå’Œresponse
                        for conv in item['conversation']:
                            if 'human' in conv and 'assistant' in conv:
                                data.append({
                                    'instruction': conv['human'],
                                    'response': conv['assistant']
                                })
                    # å¤„ç†åŸå§‹æ ¼å¼
                    elif 'instruction' in item and 'response' in item:
                        data.append(item)
                except Exception as e:
                    print(f"âš ï¸  è§£ææ•°æ®è¡Œå¤±è´¥: {e}")
                    continue

        print(f"ğŸ“ˆ åŠ è½½ {len(data)} æ¡è®­ç»ƒæ•°æ®")

        # æ·»åŠ æ•°æ®éªŒè¯
        if len(data) == 0:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆè®­ç»ƒæ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼")
            # è¿”å›ä¸€ä¸ªç©ºçš„Datasetæˆ–è€…None
            from datasets import Dataset
            return Dataset.from_dict({"text": []})

        # Qwenå¯¹è¯æ ¼å¼ - æ›´ç²¾ç¡®çš„æ ¼å¼
        def format_example(example):
            messages = [
                {"role": "user", "content": example['instruction']},
                {"role": "assistant", "content": example['response']}
            ]
            # ä½¿ç”¨tokenizerçš„apply_chat_templateæ–¹æ³•
            formatted = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False
            )
            return formatted

        formatted_texts = [format_example(item) for item in data]

        # åˆ›å»ºDataset
        from datasets import Dataset
        dataset = Dataset.from_dict({"text": formatted_texts})

        # åˆ†è¯å‡½æ•° - ä¿®å¤ç‰ˆæœ¬
        def tokenize_function(examples):
            # åªå¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œä¸æ·»åŠ ç‰¹æ®Štoken
            tokenized = tokenizer(
                examples["text"],
                truncation=True,
                max_length=self.training_config["max_length"],
                padding=False  # æ”¹ä¸ºåŠ¨æ€padding
            )

            # æ‰‹åŠ¨æ·»åŠ æ ‡ç­¾ï¼ˆç”¨äºå› æœè¯­è¨€å»ºæ¨¡ï¼‰
            tokenized["labels"] = tokenized["input_ids"].copy()
            return tokenized

        # åº”ç”¨åˆ†è¯
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=["text"]
        )

        return tokenized_dataset

    def test_trained_model(self, model_path, test_prompts=None):
        """æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹"""
        print("\nğŸ§ª æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹...")

        if test_prompts is None:
            test_prompts = [
                "ä»‹ç»ä¸€ä¸‹Pythonè¯­è¨€",
                "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
                "AMDæ˜¾å¡æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ"
            ]

        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM

            # åŠ è½½è®­ç»ƒåçš„æ¨¡å‹
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True
            )

            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float32,
                trust_remote_code=True
            ).to(self.device)

            model.eval()

            for i, prompt in enumerate(test_prompts, 1):
                print(f"\nğŸ“ æµ‹è¯• {i}: {prompt}")

                messages = [{"role": "user", "content": prompt}]
                text = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )

                inputs = tokenizer(text, return_tensors="pt").to(self.device)

                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=150,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.pad_token_id,
                    )

                    response = tokenizer.decode(
                        outputs[0][inputs["input_ids"].shape[1]:],
                        skip_special_tokens=True
                    )

                print(f"ğŸ¤– å›å¤: {response}")

        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


def main():
    """è®­ç»ƒè„šæœ¬ä¸»å…¥å£"""
    trainer = ModelTrainer()

    print("\nè¯·é€‰æ‹©è®­ç»ƒæ¨¡å¼:")
    print("1. å…¨å‚æ•°å¾®è°ƒ")
    print("2. LoRAå¾®è°ƒï¼ˆæ¨èï¼Œæ˜¾å­˜è¦æ±‚ä½ï¼‰")
    print("3. å‡†å¤‡è®­ç»ƒæ•°æ®")
    print("4. æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹")
    print("5. ğŸŒ ä¸‹è½½é­”æ­ç¤¾åŒºæ•°æ®é›†")

    choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-5): ").strip()

    if choice == "1":
        # å…¨å‚æ•°å¾®è°ƒ
        config = {
            "epochs": 3,
            "batch_size": 2,
            "learning_rate": 5e-5,
        }

        data_type = input("æ•°æ®ç±»å‹ (1=ç¤ºä¾‹, 2=æ–‡ä»¶, 3=æ‰‹åŠ¨è¾“å…¥, 4=é­”æ­æ•°æ®é›†): ").strip()
        if data_type == "1":
            data_path = None  # ä½¿ç”¨ç¤ºä¾‹æ•°æ®
        elif data_type == "2":
            data_path = input("æ•°æ®æ–‡ä»¶è·¯å¾„: ").strip()
        elif data_type == "3":
            data_path = trainer.prepare_training_data("manual")
        elif data_type == "4":
            data_path = trainer.prepare_training_data("modelscope")
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            return

        model_path = trainer.train_full_model(data_path=data_path, config=config)

        if model_path:
            test = input("æ˜¯å¦æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹ï¼Ÿ(y/n): ").strip().lower()
            if test == 'y':
                trainer.test_trained_model(model_path)

    elif choice == "2":
        # LoRAå¾®è°ƒ
        config = {
            "r": 8,
            "lora_alpha": 32,
            "epochs": 3,
            "batch_size": 4,
        }

        data_type = input("æ•°æ®ç±»å‹ (1=ç¤ºä¾‹, 2=æ–‡ä»¶, 3=é­”æ­æ•°æ®é›†): ").strip()
        if data_type == "1":
            data_path = None
        elif data_type == "2":
            data_path = input("æ•°æ®æ–‡ä»¶è·¯å¾„: ").strip()
        elif data_type == "3":
            data_path = trainer.prepare_training_data("modelscope")
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            return

        lora_path = trainer.train_lora(data_path=data_path, config=config)

        if lora_path:
            print(f"\nğŸ’¡ LoRAæƒé‡è·¯å¾„: {lora_path}")
            print("ğŸ’¡ è¦åœ¨æ¨ç†ä»£ç ä¸­ä½¿ç”¨LoRAï¼Œéœ€è¦:")
            print("  1. åŠ è½½åŸºç¡€æ¨¡å‹")
            print("  2. ä½¿ç”¨PeftModelåŠ è½½LoRAæƒé‡")
            print("  3. ä½¿ç”¨model.merge_and_unload()åˆå¹¶æƒé‡")

    elif choice == "3":
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        print("\nå‡†å¤‡è®­ç»ƒæ•°æ®:")
        print("1. åˆ›å»ºç¤ºä¾‹æ•°æ®")
        print("2. æ‰‹åŠ¨è¾“å…¥æ•°æ®")

        sub_choice = input("è¯·é€‰æ‹©: ").strip()
        if sub_choice == "1":
            num_samples = input("æ ·æœ¬æ•°é‡ (é»˜è®¤20): ").strip()
            num_samples = int(num_samples) if num_samples else 20
            trainer.prepare_training_data("example")
        elif sub_choice == "2":
            trainer.prepare_training_data("manual")

    elif choice == "4":
        # æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹
        model_path = input("è®­ç»ƒæ¨¡å‹è·¯å¾„: ").strip()
        if os.path.exists(model_path):
            trainer.test_trained_model(model_path)
        else:
            print("âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨")

    elif choice == "5":
        # ä¸‹è½½é­”æ­ç¤¾åŒºæ•°æ®é›†
        data_path = trainer.prepare_training_data("modelscope")
        if data_path:
            print(f"\nâœ… æ•°æ®é›†å·²ä¸‹è½½åˆ°: {data_path}")
            use_now = input("æ˜¯å¦ç«‹å³ä½¿ç”¨æ­¤æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Ÿ(y/n): ").strip().lower()
            if use_now == 'y':
                # è¯¢é—®è®­ç»ƒç±»å‹
                print("\né€‰æ‹©è®­ç»ƒç±»å‹:")
                print("1. å…¨å‚æ•°å¾®è°ƒ")
                print("2. LoRAå¾®è°ƒ")

                train_choice = input("è¯·é€‰æ‹© (1/2): ").strip()

                if train_choice == "1":
                    config = {
                        "epochs": 3,
                        "batch_size": 2,
                        "learning_rate": 5e-5,
                    }
                    trainer.train_full_model(data_path=data_path, config=config)
                elif train_choice == "2":
                    config = {
                        "r": 8,
                        "lora_alpha": 32,
                        "epochs": 3,
                        "batch_size": 4,
                    }
                    trainer.train_lora(data_path=data_path, config=config)
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©")

    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")


if __name__ == "__main__":
    # æ£€æŸ¥modelscopeæ˜¯å¦å·²å®‰è£…
    try:
        import modelscope

        print("âœ… modelscopeåº“å·²å®‰è£…")
    except ImportError:
        print("âš ï¸  modelscopeåº“æœªå®‰è£…")
        install = input("æ˜¯å¦ç°åœ¨å®‰è£…ï¼Ÿ(y/n): ").strip().lower()
        if install == 'y':
            try:
                subprocess.check_call([sys.executable, "-m", "pip", "install", "modelscope"])
                print("âœ… modelscopeå®‰è£…æˆåŠŸ")
            except:
                print("âŒ å®‰è£…å¤±è´¥ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½æ— æ³•ä½¿ç”¨")

    main()